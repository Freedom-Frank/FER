# ===============================================================================================
# The following shows the last analyze fail log message.
# ===============================================================================================

----------------------------------------------------
- Caught exception:
----------------------------------------------------
Features shape length must be equal to Labels shape length plus 1

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\ops\sparse_softmax_cross_entropy_with_logits.cc:45 mindspore::ops::`anonymous-namespace'::SparseSoftmaxCrossEntropyWithLogitsInferShape

----------------------------------------------------
- The Traceback of Net Construct Code:
----------------------------------------------------
# 0 In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417
        if not self.sense_flag:
# 1 In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418
            return self._no_sens_impl(*inputs)
                   ^
# 2 In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437
        if self.return_grad:
# 3 In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433
        loss = self.network(*inputs)
               ^
# 4 In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122
        return self._loss_fn(out, label)
               ^
# 5 In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:780
        if self.sparse:
# 6 In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781
            if self.reduction == 'mean':
            ^
# 7 In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:782
                x = self.sparse_softmax_cross_entropy(logits, labels)
                    ^

# ===============================================================================================
# The following shows the IR when the function graphs evaluation fails to help locate the problem.
# You can search the last ------------------------> to the node which is evaluated failure.
# Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================================

# IR entry: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13
# Total subgraphs: 231

# Attrs:
training : 1

# Total params: 51
# Params:
%para1_inputs0 : <null>
%para2_inputs1 : <null>
%para3_features.0.weight : <Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:features.0.weight>  :  has_default
%para4_features.1.gamma : <Ref[Tensor[Float32]], (32), ref_key=:features.1.gamma>  :  has_default
%para5_features.1.beta : <Ref[Tensor[Float32]], (32), ref_key=:features.1.beta>  :  has_default
%para6_features.4.weight : <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:features.4.weight>  :  has_default
%para7_features.5.gamma : <Ref[Tensor[Float32]], (64), ref_key=:features.5.gamma>  :  has_default
%para8_features.5.beta : <Ref[Tensor[Float32]], (64), ref_key=:features.5.beta>  :  has_default
%para9_features.8.weight : <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:features.8.weight>  :  has_default
%para10_features.9.gamma : <Ref[Tensor[Float32]], (128), ref_key=:features.9.gamma>  :  has_default
%para11_features.9.beta : <Ref[Tensor[Float32]], (128), ref_key=:features.9.beta>  :  has_default
%para12_classifier.0.weight : <Ref[Tensor[Float32]], (128, 128), ref_key=:classifier.0.weight>  :  has_default
%para13_classifier.0.bias : <Ref[Tensor[Float32]], (128), ref_key=:classifier.0.bias>  :  has_default
%para14_classifier.3.weight : <Ref[Tensor[Float32]], (7, 128), ref_key=:classifier.3.weight>  :  has_default
%para15_classifier.3.bias : <Ref[Tensor[Float32]], (7), ref_key=:classifier.3.bias>  :  has_default
%para16_moment1.features.0.weight : <Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:moment1.features.0.weight>  :  has_default
%para17_moment1.features.1.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment1.features.1.gamma>  :  has_default
%para18_moment1.features.1.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment1.features.1.beta>  :  has_default
%para19_moment1.features.4.weight : <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:moment1.features.4.weight>  :  has_default
%para20_moment1.features.5.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moment1.features.5.gamma>  :  has_default
%para21_moment1.features.5.beta : <Ref[Tensor[Float32]], (64), ref_key=:moment1.features.5.beta>  :  has_default
%para22_moment1.features.8.weight : <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:moment1.features.8.weight>  :  has_default
%para23_moment1.features.9.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.9.gamma>  :  has_default
%para24_moment1.features.9.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.9.beta>  :  has_default
%para25_moment1.classifier.0.weight : <Ref[Tensor[Float32]], (128, 128), ref_key=:moment1.classifier.0.weight>  :  has_default
%para26_moment1.classifier.0.bias : <Ref[Tensor[Float32]], (128), ref_key=:moment1.classifier.0.bias>  :  has_default
%para27_moment1.classifier.3.weight : <Ref[Tensor[Float32]], (7, 128), ref_key=:moment1.classifier.3.weight>  :  has_default
%para28_moment1.classifier.3.bias : <Ref[Tensor[Float32]], (7), ref_key=:moment1.classifier.3.bias>  :  has_default
%para29_moment2.features.0.weight : <Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:moment2.features.0.weight>  :  has_default
%para30_moment2.features.1.gamma : <Ref[Tensor[Float32]], (32), ref_key=:moment2.features.1.gamma>  :  has_default
%para31_moment2.features.1.beta : <Ref[Tensor[Float32]], (32), ref_key=:moment2.features.1.beta>  :  has_default
%para32_moment2.features.4.weight : <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:moment2.features.4.weight>  :  has_default
%para33_moment2.features.5.gamma : <Ref[Tensor[Float32]], (64), ref_key=:moment2.features.5.gamma>  :  has_default
%para34_moment2.features.5.beta : <Ref[Tensor[Float32]], (64), ref_key=:moment2.features.5.beta>  :  has_default
%para35_moment2.features.8.weight : <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:moment2.features.8.weight>  :  has_default
%para36_moment2.features.9.gamma : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.9.gamma>  :  has_default
%para37_moment2.features.9.beta : <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.9.beta>  :  has_default
%para38_moment2.classifier.0.weight : <Ref[Tensor[Float32]], (128, 128), ref_key=:moment2.classifier.0.weight>  :  has_default
%para39_moment2.classifier.0.bias : <Ref[Tensor[Float32]], (128), ref_key=:moment2.classifier.0.bias>  :  has_default
%para40_moment2.classifier.3.weight : <Ref[Tensor[Float32]], (7, 128), ref_key=:moment2.classifier.3.weight>  :  has_default
%para41_moment2.classifier.3.bias : <Ref[Tensor[Float32]], (7), ref_key=:moment2.classifier.3.bias>  :  has_default
%para42_beta1_power : <Ref[Tensor[Float32]], (), ref_key=:beta1_power>  :  has_default
%para43_beta2_power : <Ref[Tensor[Float32]], (), ref_key=:beta2_power>  :  has_default
%para44_global_step : <Ref[Tensor[Int32]], (1), ref_key=:global_step>  :  has_default
%para45_learning_rate : <Ref[Tensor[Float32]], (), ref_key=:learning_rate>  :  has_default
%para46_features.1.moving_mean : <Ref[Tensor[Float32]], (32), ref_key=:features.1.moving_mean>  :  has_default
%para47_features.1.moving_variance : <Ref[Tensor[Float32]], (32), ref_key=:features.1.moving_variance>  :  has_default
%para48_features.5.moving_mean : <Ref[Tensor[Float32]], (64), ref_key=:features.5.moving_mean>  :  has_default
%para49_features.5.moving_variance : <Ref[Tensor[Float32]], (64), ref_key=:features.5.moving_variance>  :  has_default
%para50_features.9.moving_mean : <Ref[Tensor[Float32]], (128), ref_key=:features.9.moving_mean>  :  has_default
%para51_features.9.moving_variance : <Ref[Tensor[Float32]], (128), ref_key=:features.9.moving_variance>  :  has_default

subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13 : 000001EBE7090480
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13(%para1_inputs0, %para2_inputs1, %para3_features.0.weight, %para4_features.1.gamma, %para5_features.1.beta, %para6_features.4.weight, %para7_features.5.gamma, %para8_features.5.beta, %para9_features.8.weight, %para10_features.9.gamma, %para11_features.9.beta, %para12_classifier.0.weight, %para13_classifier.0.bias, %para14_classifier.3.weight, %para15_classifier.3.bias, %para16_moment1.features.0.weight, %para17_moment1.features.1.gamma, %para18_moment1.features.1.beta, %para19_moment1.features.4.weight, %para20_moment1.features.5.gamma, %para21_moment1.features.5.beta, %para22_moment1.features.8.weight, %para23_moment1.features.9.gamma, %para24_moment1.features.9.beta, %para25_moment1.classifier.0.weight, %para26_moment1.classifier.0.bias, %para27_moment1.classifier.3.weight, %para28_moment1.classifier.3.bias, %para29_moment2.features.0.weight, %para30_moment2.features.1.gamma, %para31_moment2.features.1.beta, %para32_moment2.features.4.weight, %para33_moment2.features.5.gamma, %para34_moment2.features.5.beta, %para35_moment2.features.8.weight, %para36_moment2.features.9.gamma, %para37_moment2.features.9.beta, %para38_moment2.classifier.0.weight, %para39_moment2.classifier.0.bias, %para40_moment2.classifier.3.weight, %para41_moment2.classifier.3.bias, %para42_beta1_power, %para43_beta2_power, %para44_global_step, %para45_learning_rate, %para46_features.1.moving_mean, %para47_features.1.moving_variance, %para48_features.5.moving_mean, %para49_features.5.moving_variance, %para50_features.9.moving_mean, %para51_features.9.moving_variance) {

#------------------------> 0
  %1(CNode_28) = call @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_14()
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417/        if not self.sense_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417/        if not self.sense_flag:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13:CNode_28{[0]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_14}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13:CNode_29{[0]: ValueNode<Primitive> Return, [1]: CNode_28}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_14 : 000001EBE7094440
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_14 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13]() {
  %1(CNode_30) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13):MakeTuple(%para1_inputs0, %para2_inputs1)
      : (<Tensor[Float32], (64, 1, 48, 48)>, <Tensor[Int32], (64, 1)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((64, 1, 48, 48), (64, 1))>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/

#------------------------> 1
  %2(CNode_31) = UnpackCall_unpack_call(@_no_sens_impl_32, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((64, 1, 48, 48), (64, 1))>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%2)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_14:CNode_31{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.33, [1]: ValueNode<FuncGraph> _no_sens_impl_32, [2]: CNode_30}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_14:CNode_34{[0]: ValueNode<Primitive> Return, [1]: CNode_31}


subgraph attr:
core : 1
subgraph instance: UnpackCall_15 : 000001EBEA0534E0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
subgraph @UnpackCall_15(%para52_, %para53_) {
  %1(CNode_31) = TupleGetItem(%para53_17, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((64, 1, 48, 48), (64, 1))>, <Int64, NoShape>) -> (<Tensor[Float32], (64, 1, 48, 48)>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  %2(CNode_31) = TupleGetItem(%para53_17, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((64, 1, 48, 48), (64, 1))>, <Int64, NoShape>) -> (<Tensor[Int32], (64, 1)>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/

#------------------------> 2
  %3(CNode_31) = %para52_16(%1, %2)
      : (<Tensor[Float32], (64, 1, 48, 48)>, <Tensor[Int32], (64, 1)>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @UnpackCall_15:CNode_31{[0]: param_16, [1]: CNode_31, [2]: CNode_31}
#   2: @UnpackCall_15:CNode_31{[0]: ValueNode<Primitive> Return, [1]: CNode_31}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_18 : 000001EBEA053A30
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_18 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para54_inputs0, %para55_inputs1) {

#------------------------> 3
  %1(CNode_35) = call @_no_sens_impl_19()
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_18:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.36, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_24, [2]: CNode_37}
#   2: @_no_sens_impl_18:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_24, [2]: CNode_37}
#   3: @_no_sens_impl_18:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_38}
#   4: @_no_sens_impl_18:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.39, [1]: grads, [2]: CNode_37}
#   5: @_no_sens_impl_18:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_40, [1]: grads}
#   6: @_no_sens_impl_18:CNode_41{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_42, [1]: grads}
#   7: @_no_sens_impl_18:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_41}
#   8: @_no_sens_impl_18:CNode_35{[0]: ValueNode<FuncGraph> _no_sens_impl_19}
#   9: @_no_sens_impl_18:CNode_43{[0]: ValueNode<Primitive> Return, [1]: CNode_35}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_19 : 000001EBEA04FFC0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_19 parent: [subgraph @_no_sens_impl_18]() {

#------------------------> 4
  %1(CNode_44) = call @_no_sens_impl_20()
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_19:CNode_44{[0]: ValueNode<FuncGraph> _no_sens_impl_20}
#   2: @_no_sens_impl_19:CNode_45{[0]: ValueNode<Primitive> Return, [1]: CNode_44}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_20 : 000001EBEA0564B0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_20 parent: [subgraph @_no_sens_impl_18]() {
  %1(CNode_37) = $(_no_sens_impl_18):MakeTuple(%para54_inputs0, %para55_inputs1)
      : (<Tensor[Float32], (64, 1, 48, 48)>, <Tensor[Int32], (64, 1)>) -> (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((64, 1, 48, 48), (64, 1))>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/

#------------------------> 5
  %2(loss) = $(_no_sens_impl_18):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_24, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((64, 1, 48, 48), (64, 1))>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  %3(grads) = $(_no_sens_impl_18):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_24, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((64, 1, 48, 48), (64, 1))>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(CNode_38) = $(_no_sens_impl_18):MakeTuple(%para3_features.0.weight, %para4_features.1.gamma, %para5_features.1.beta, %para6_features.4.weight, %para7_features.5.gamma, %para8_features.5.beta, %para9_features.8.weight, %para10_features.9.gamma, %para11_features.9.beta, %para12_classifier.0.weight, %para13_classifier.0.bias, %para14_classifier.3.weight, %para15_classifier.3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (32)>, <Ref[Tensor[Float32]], (64, 32, 3, 3)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (64)>, <Ref[Tensor[Float32]], (128, 64, 3, 3)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (128, 128)>, <Ref[Tensor[Float32]], (128)>, <Ref[Tensor[Float32]], (7, 128)>, <Ref[Tensor[Float32]], (7)>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_18):S_Prim_grad(%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_18):UnpackCall_unpack_call(%5, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((64, 1, 48, 48), (64, 1))>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %7(grads) = $(_no_sens_impl_18):call @mindspore_nn_layer_basic_Identity_construct_40(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %8(CNode_41) = $(_no_sens_impl_18):call @mindspore_nn_optim_adam_Adam_construct_42(%7)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %9(loss) = $(_no_sens_impl_18):S_Prim_Depend[side_effect_propagate: I64(1)](%2, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%9)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @_no_sens_impl_20:CNode_46{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
core : 1
subgraph instance: UnpackCall_21 : 000001EBDD095AF0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
subgraph @UnpackCall_21(%para56_, %para57_) {
  %1(loss) = TupleGetItem(%para57_23, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((64, 1, 48, 48), (64, 1))>, <Int64, NoShape>) -> (<Tensor[Float32], (64, 1, 48, 48)>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(loss) = TupleGetItem(%para57_23, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[Int32]], TupleShape((64, 1, 48, 48), (64, 1))>, <Int64, NoShape>) -> (<Tensor[Int32], (64, 1)>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/

#------------------------> 6
  %3(loss) = %para56_22(%1, %2)
      : (<Tensor[Float32], (64, 1, 48, 48)>, <Tensor[Int32], (64, 1)>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
}
# Order:
#   1: @UnpackCall_21:loss{[0]: param_22, [1]: loss, [2]: loss}
#   2: @UnpackCall_21:loss{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_24 : 000001EBEA0554C0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_24 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para58_data, %para59_label) {
  %1(out) = call @model_SimpleCNN_construct_47(%para58_data)
      : (<Tensor[Float32], (64, 1, 48, 48)>) -> (<Tensor[Float32], (64, 7)>)
      #scope: (Default/network-WithLossCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:121/        out = self._backbone(data)/

#------------------------> 7
  %2(CNode_48) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25(%1, %para59_label)
      : (<Tensor[Float32], (64, 7)>, <Tensor[Int32], (64, 1)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_24:out{[0]: ValueNode<FuncGraph> model_SimpleCNN_construct_47, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_24:CNode_48{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_24:CNode_49{[0]: ValueNode<Primitive> Return, [1]: CNode_48}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25 : 000001EBDD0C8D10
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25(%para60_logits, %para61_labels) {
  %1(CNode_50) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("logits", %para60_logits, "SoftmaxCrossEntropyWithLogits")
      : (<String, NoShape>, <Tensor[Float32], (64, 7)>, <String, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:778/        _check_is_tensor('logits', logits, self.cls_name)/
  %2(CNode_51) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("labels", %para61_labels, "SoftmaxCrossEntropyWithLogits")
      : (<String, NoShape>, <Tensor[Int32], (64, 1)>, <String, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:779/        _check_is_tensor('labels', labels, self.cls_name)/
  %3(CNode_52) = MakeTuple(%1, %2)
      : (<None, NoShape>, <None, NoShape>) -> (<Tuple[None*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
  %4(CNode_53) = StopGradient(%3)
      : (<Tuple[None*2], TupleShape(NoShape, NoShape)>) -> (<Tuple[None*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/

#------------------------> 8
  %5(CNode_54) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_26()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  %6(CNode_55) = Depend[side_effect_propagate: I64(1)](%5, %4)
      : (<null>, <Tuple[None*2], TupleShape(NoShape, NoShape)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25:CNode_50{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> logits, [2]: param_logits, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25:CNode_51{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> labels, [2]: param_labels, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25:CNode_54{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_26}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25:CNode_56{[0]: ValueNode<Primitive> Return, [1]: CNode_55}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_26 : 000001EBDD0C87C0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_26 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25]() {
  %1(CNode_57) = S_Prim_equal("mean", "mean")
      : (<String, NoShape>, <String, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %2(CNode_58) = Cond(%1, Bool(0))
      : (<Bool, NoShape>, <Bool, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %3(CNode_59) = Switch(%2, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_27, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_60)
      : (<Bool, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/

#------------------------> 9
  %4(CNode_61) = %3()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_26:CNode_57{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<StringImm> mean, [2]: ValueNode<StringImm> mean}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_26:CNode_58{[0]: ValueNode<Primitive> Cond, [1]: CNode_57, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_26:CNode_59{[0]: ValueNode<Primitive> Switch, [1]: CNode_58, [2]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_27, [3]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_60}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_26:CNode_61{[0]: CNode_59}
#   5: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_26:CNode_62{[0]: ValueNode<Primitive> Return, [1]: CNode_61}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_27 : 000001EBDD0CCCD0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_27 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25]() {

#------------------------> 10
  %1(x) = S_Prim_SparseSoftmaxCrossEntropyWithLogits[is_grad: Bool(0), sens: F32(1), input_names: ["features", "labels"], output_names: ["output"]](%para60_logits, %para61_labels)
      : (<Tensor[Float32], (64, 7)>, <Tensor[Int32], (64, 1)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:782/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:783/                return x/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_27:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SparseSoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: param_labels}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_27:CNode_63{[0]: ValueNode<Primitive> Return, [1]: x}


# ===============================================================================================
# The total of function graphs in evaluation stack: 11/12 (Ignored 1 internal frames).
# ===============================================================================================


# ===============================================================================================
# The rest function graphs are the following:
# ===============================================================================================
subgraph attr:
training : 1
subgraph instance: _no_sens_impl_32 : 000001EBE70909D0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_32 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para62_inputs) {
  %1(CNode_35) = call @_no_sens_impl_64()
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_32:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.36, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_65, [2]: param_inputs}
#   2: @_no_sens_impl_32:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_65, [2]: param_inputs}
#   3: @_no_sens_impl_32:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_38}
#   4: @_no_sens_impl_32:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.39, [1]: grads, [2]: param_inputs}
#   5: @_no_sens_impl_32:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_66, [1]: grads}
#   6: @_no_sens_impl_32:CNode_41{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_67, [1]: grads}
#   7: @_no_sens_impl_32:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_41}
#   8: @_no_sens_impl_32:CNode_35{[0]: ValueNode<FuncGraph> _no_sens_impl_64}
#   9: @_no_sens_impl_32:CNode_43{[0]: ValueNode<Primitive> Return, [1]: CNode_35}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_67 : 000001EBE7091F10
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:910/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_adam_Adam_construct_67 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para63_gradients) {
  %1(CNode_69) = call @mindspore_nn_optim_adam_Adam_construct_68()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:916/        if not self.use_offload:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:916/        if not self.use_offload:/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_67:gradients{[0]: ValueNode<FuncGraph> flatten_gradients_70, [1]: param_gradients}
#   2: @mindspore_nn_optim_adam_Adam_construct_67:gradients{[0]: ValueNode<FuncGraph> decay_weight_71, [1]: gradients}
#   3: @mindspore_nn_optim_adam_Adam_construct_67:CNode_69{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_68}
#   4: @mindspore_nn_optim_adam_Adam_construct_67:CNode_72{[0]: ValueNode<Primitive> Return, [1]: CNode_69}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Identity_construct_66 : 000001EBE70919C0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:505/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Identity_construct_66(%para64_x) {
  Return(%para64_x)
      : (<null>)
      #scope: (Default/grad_reducer-Identity)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:506/        return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Identity_construct_66:CNode_73{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_65 : 000001EBE707FAE0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_65 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para65_data, %para66_label) {
  %1(out) = call @model_SimpleCNN_construct_74(%para65_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_48) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_75(%1, %para66_label)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_65:out{[0]: ValueNode<FuncGraph> model_SimpleCNN_construct_74, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_65:CNode_48{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_75, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_65:CNode_49{[0]: ValueNode<Primitive> Return, [1]: CNode_48}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_64 : 000001EBEA052A40
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_64 parent: [subgraph @_no_sens_impl_32]() {
  %1(CNode_44) = call @_no_sens_impl_76()
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_64:CNode_44{[0]: ValueNode<FuncGraph> _no_sens_impl_76}
#   2: @_no_sens_impl_64:CNode_45{[0]: ValueNode<Primitive> Return, [1]: CNode_44}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_71 : 000001EBE7093450
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_71(%para67_gradients) {
  %1(CNode_78) = call @decay_weight_77()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @decay_weight_71:CNode_78{[0]: ValueNode<FuncGraph> decay_weight_77}
#   2: @decay_weight_71:CNode_79{[0]: ValueNode<Primitive> Return, [1]: CNode_78}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_70 : 000001EBE7094990
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_70(%para68_gradients) {
  %1(CNode_81) = call @flatten_gradients_80()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_70:CNode_81{[0]: ValueNode<FuncGraph> flatten_gradients_80}
#   2: @flatten_gradients_70:CNode_82{[0]: ValueNode<Primitive> Return, [1]: CNode_81}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_68 : 000001EBE7098400
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:910/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_adam_Adam_construct_68 parent: [subgraph @mindspore_nn_optim_adam_Adam_construct_67]() {
  %1(CNode_84) = call @mindspore_nn_optim_adam_Adam_construct_83()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:917/            gradients = self.gradients_centralization(gradients)/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:917/            gradients = self.gradients_centralization(gradients)/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_68:gradients{[0]: ValueNode<FuncGraph> gradients_centralization_85, [1]: gradients}
#   2: @mindspore_nn_optim_adam_Adam_construct_68:CNode_84{[0]: ValueNode<FuncGraph> mindspore_nn_optim_adam_Adam_construct_83}
#   3: @mindspore_nn_optim_adam_Adam_construct_68:CNode_86{[0]: ValueNode<Primitive> Return, [1]: CNode_84}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_75 : 000001EBEA04A020
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_75(%para69_logits, %para70_labels) {
  %1(CNode_50) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("logits", %para69_logits, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:778/        _check_is_tensor('logits', logits, self.cls_name)/
  %2(CNode_51) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("labels", %para70_labels, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:779/        _check_is_tensor('labels', labels, self.cls_name)/
  %3(CNode_52) = MakeTuple(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
  %4(CNode_53) = StopGradient(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
  %5(CNode_54) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_87()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  %6(CNode_55) = Depend[side_effect_propagate: I64(1)](%5, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_75:CNode_50{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> logits, [2]: param_logits, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_75:CNode_51{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> labels, [2]: param_labels, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_75:CNode_54{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_87}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_75:CNode_56{[0]: ValueNode<Primitive> Return, [1]: CNode_55}


subgraph attr:
training : 1
subgraph instance: model_SimpleCNN_construct_74 : 000001EBE707C070
# In file D:\BUAA\25-26 Front\MindSpore\model.py:44/    def construct(self, x):/
subgraph @model_SimpleCNN_construct_74 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para71_x) {
  %1(x) = call @mindspore_nn_layer_container_SequentialCell_construct_88(%para71_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN)
      # In file D:\BUAA\25-26 Front\MindSpore\model.py:45/        x = self.features(x)/
  %2(x) = call @mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_89(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN)
      # In file D:\BUAA\25-26 Front\MindSpore\model.py:46/        x = self.global_pool(x)/
  %3(x) = call @mindspore_nn_layer_basic_Flatten_construct_90(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN)
      # In file D:\BUAA\25-26 Front\MindSpore\model.py:47/        x = self.flatten(x)/
  %4(x) = call @mindspore_nn_layer_container_SequentialCell_construct_91(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN)
      # In file D:\BUAA\25-26 Front\MindSpore\model.py:48/        x = self.classifier(x)/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN)
      # In file D:\BUAA\25-26 Front\MindSpore\model.py:49/        return x/
}
# Order:
#   1: @model_SimpleCNN_construct_74:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_88, [1]: param_x}
#   2: @model_SimpleCNN_construct_74:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_89, [1]: x}
#   3: @model_SimpleCNN_construct_74:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_90, [1]: x}
#   4: @model_SimpleCNN_construct_74:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_91, [1]: x}
#   5: @model_SimpleCNN_construct_74:CNode_92{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_76 : 000001EBEA04FA70
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_76 parent: [subgraph @_no_sens_impl_32]() {
  %1(loss) = $(_no_sens_impl_32):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_65, %para62_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(grads) = $(_no_sens_impl_32):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_65, %para62_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %3(CNode_38) = $(_no_sens_impl_32):MakeTuple(%para3_features.0.weight, %para4_features.1.gamma, %para5_features.1.beta, %para6_features.4.weight, %para7_features.5.gamma, %para8_features.5.beta, %para9_features.8.weight, %para10_features.9.gamma, %para11_features.9.beta, %para12_classifier.0.weight, %para13_classifier.0.bias, %para14_classifier.3.weight, %para15_classifier.3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:features.0.weight>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.beta>, <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:features.4.weight>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.beta>, <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:features.8.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.beta>, <Ref[Tensor[Float32]], (128, 128), ref_key=:classifier.0.weight>, <Ref[Tensor[Float32]], (128), ref_key=:classifier.0.bias>, <Ref[Tensor[Float32]], (7, 128), ref_key=:classifier.3.weight>, <Ref[Tensor[Float32]], (7), ref_key=:classifier.3.bias>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(grads) = $(_no_sens_impl_32):S_Prim_grad(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_32):UnpackCall_unpack_call(%4, %para62_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_32):call @mindspore_nn_layer_basic_Identity_construct_66(%5)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %7(CNode_41) = $(_no_sens_impl_32):call @mindspore_nn_optim_adam_Adam_construct_67(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %8(loss) = $(_no_sens_impl_32):S_Prim_Depend[side_effect_propagate: I64(1)](%1, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%8)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @_no_sens_impl_76:CNode_46{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_77 : 000001EBE7090F20
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_77 parent: [subgraph @decay_weight_71]() {
  %1(CNode_94) = call @decay_weight_93()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @decay_weight_77:CNode_94{[0]: ValueNode<FuncGraph> decay_weight_93}
#   2: @decay_weight_77:CNode_95{[0]: ValueNode<Primitive> Return, [1]: CNode_94}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_80 : 000001EBE7092F00
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_80 parent: [subgraph @flatten_gradients_70]() {
  %1(CNode_97) = call @flatten_gradients_96()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_80:CNode_97{[0]: ValueNode<FuncGraph> flatten_gradients_96}
#   2: @flatten_gradients_80:CNode_98{[0]: ValueNode<Primitive> Return, [1]: CNode_97}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_85 : 000001EBE7095ED0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_85(%para72_gradients) {
  %1(CNode_100) = call @gradients_centralization_99()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_85:CNode_100{[0]: ValueNode<FuncGraph> gradients_centralization_99}
#   2: @gradients_centralization_85:CNode_101{[0]: ValueNode<Primitive> Return, [1]: CNode_100}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_adam_Adam_construct_83 : 000001EBE7097410
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:910/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_adam_Adam_construct_83 parent: [subgraph @mindspore_nn_optim_adam_Adam_construct_68]() {
  %1(CNode_102) = S_Prim_AssignAdd[input_names: ["ref", "value"], output_names: ["ref"], side_effect_mem: Bool(1)](%para44_global_step, Tensor(shape=[1], dtype=Int32, value=[1]))
      : (<Ref[Tensor[Int32]], (1), ref_key=:global_step>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:921/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %2(beta1_power) = S_Prim_mul(%para42_beta1_power, Tensor(shape=[], dtype=Float32, value=0.9))
      : (<Ref[Tensor[Float32]], (), ref_key=:beta1_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:923/        beta1_power = self.beta1_power * self.beta1/
  %3(CNode_104) = call @assign_103(%para42_beta1_power, %2)
      : (<Ref[Tensor[Float32]], (), ref_key=:beta1_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:924/        self.beta1_power = beta1_power/
  %4(beta2_power) = S_Prim_mul(%para43_beta2_power, Tensor(shape=[], dtype=Float32, value=0.999))
      : (<Ref[Tensor[Float32]], (), ref_key=:beta2_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:925/        beta2_power = self.beta2_power * self.beta2/
  %5(CNode_105) = call @assign_103(%para43_beta2_power, %4)
      : (<Ref[Tensor[Float32]], (), ref_key=:beta2_power>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:926/        self.beta2_power = beta2_power/
  %6(CNode_106) = MakeTuple(%1, %3, %5)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:909/    @jit/
  %7(CNode_107) = StopGradient(%6)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:909/    @jit/
  %8(CNode_108) = $(mindspore_nn_optim_adam_Adam_construct_67):MakeTuple(%para3_features.0.weight, %para4_features.1.gamma, %para5_features.1.beta, %para6_features.4.weight, %para7_features.5.gamma, %para8_features.5.beta, %para9_features.8.weight, %para10_features.9.gamma, %para11_features.9.beta, %para12_classifier.0.weight, %para13_classifier.0.bias, %para14_classifier.3.weight, %para15_classifier.3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:features.0.weight>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.beta>, <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:features.4.weight>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.beta>, <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:features.8.weight>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.beta>, <Ref[Tensor[Float32]], (128, 128), ref_key=:classifier.0.weight>, <Ref[Tensor[Float32]], (128), ref_key=:classifier.0.bias>, <Ref[Tensor[Float32]], (7, 128), ref_key=:classifier.3.weight>, <Ref[Tensor[Float32]], (7), ref_key=:classifier.3.bias>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:911/        params = self._parameters/
  %9(CNode_109) = $(mindspore_nn_optim_adam_Adam_construct_67):MakeTuple(%para16_moment1.features.0.weight, %para17_moment1.features.1.gamma, %para18_moment1.features.1.beta, %para19_moment1.features.4.weight, %para20_moment1.features.5.gamma, %para21_moment1.features.5.beta, %para22_moment1.features.8.weight, %para23_moment1.features.9.gamma, %para24_moment1.features.9.beta, %para25_moment1.classifier.0.weight, %para26_moment1.classifier.0.bias, %para27_moment1.classifier.3.weight, %para28_moment1.classifier.3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:moment1.features.0.weight>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.features.1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment1.features.1.beta>, <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:moment1.features.4.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moment1.features.5.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moment1.features.5.beta>, <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:moment1.features.8.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.9.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.features.9.beta>, <Ref[Tensor[Float32]], (128, 128), ref_key=:moment1.classifier.0.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment1.classifier.0.bias>, <Ref[Tensor[Float32]], (7, 128), ref_key=:moment1.classifier.3.weight>, <Ref[Tensor[Float32]], (7), ref_key=:moment1.classifier.3.bias>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:912/        moment1 = self.moment1/
  %10(CNode_110) = $(mindspore_nn_optim_adam_Adam_construct_67):MakeTuple(%para29_moment2.features.0.weight, %para30_moment2.features.1.gamma, %para31_moment2.features.1.beta, %para32_moment2.features.4.weight, %para33_moment2.features.5.gamma, %para34_moment2.features.5.beta, %para35_moment2.features.8.weight, %para36_moment2.features.9.gamma, %para37_moment2.features.9.beta, %para38_moment2.classifier.0.weight, %para39_moment2.classifier.0.bias, %para40_moment2.classifier.3.weight, %para41_moment2.classifier.3.bias)
      : (<Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:moment2.features.0.weight>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.features.1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:moment2.features.1.beta>, <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:moment2.features.4.weight>, <Ref[Tensor[Float32]], (64), ref_key=:moment2.features.5.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:moment2.features.5.beta>, <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:moment2.features.8.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.9.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.features.9.beta>, <Ref[Tensor[Float32]], (128, 128), ref_key=:moment2.classifier.0.weight>, <Ref[Tensor[Float32]], (128), ref_key=:moment2.classifier.0.bias>, <Ref[Tensor[Float32]], (7, 128), ref_key=:moment2.classifier.3.weight>, <Ref[Tensor[Float32]], (7), ref_key=:moment2.classifier.3.bias>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:913/        moment2 = self.moment2/
  %11(lr) = call @get_lr_111()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:920/        lr = self.get_lr()/
  %12(gradients) = $(mindspore_nn_optim_adam_Adam_construct_67):call @flatten_gradients_70(%para63_gradients)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:914/        gradients = self.flatten_gradients(gradients)/
  %13(gradients) = $(mindspore_nn_optim_adam_Adam_construct_67):call @decay_weight_71(%12)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:915/        gradients = self.decay_weight(gradients)/
  %14(gradients) = $(mindspore_nn_optim_adam_Adam_construct_68):call @gradients_centralization_85(%13)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:917/            gradients = self.gradients_centralization(gradients)/
  %15(gradients) = call @scale_grad_112(%14)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:918/        gradients = self.scale_grad(gradients)/
  %16(gradients) = call @_grad_sparse_indices_deduplicate_113(%15)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:919/        gradients = self._grad_sparse_indices_deduplicate(gradients)/
  %17(CNode_115) = call @_apply_adam_114(%8, %2, %4, %9, %10, %11, %16)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:928/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  %18(CNode_116) = Depend[side_effect_propagate: I64(1)](%17, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:928/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
  Return(%18)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:928/        return self._apply_adam(params, beta1_power, beta2_power, moment1, moment2, lr, gradients)/
}
# Order:
#   1: @mindspore_nn_optim_adam_Adam_construct_83:gradients{[0]: ValueNode<FuncGraph> scale_grad_112, [1]: gradients}
#   2: @mindspore_nn_optim_adam_Adam_construct_83:gradients{[0]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_113, [1]: gradients}
#   3: @mindspore_nn_optim_adam_Adam_construct_83:lr{[0]: ValueNode<FuncGraph> get_lr_111}
#   4: @mindspore_nn_optim_adam_Adam_construct_83:CNode_102{[0]: ValueNode<DoSignaturePrimitive> S_Prim_AssignAdd, [1]: param_global_step, [2]: ValueNode<Tensor> Tensor(shape=[1], dtype=Int32, value=[1])}
#   5: @mindspore_nn_optim_adam_Adam_construct_83:beta1_power{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_beta1_power, [2]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.9)}
#   6: @mindspore_nn_optim_adam_Adam_construct_83:CNode_104{[0]: ValueNode<FuncGraph> assign_103, [1]: param_beta1_power, [2]: beta1_power}
#   7: @mindspore_nn_optim_adam_Adam_construct_83:beta2_power{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_beta2_power, [2]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.999)}
#   8: @mindspore_nn_optim_adam_Adam_construct_83:CNode_105{[0]: ValueNode<FuncGraph> assign_103, [1]: param_beta2_power, [2]: beta2_power}
#   9: @mindspore_nn_optim_adam_Adam_construct_83:CNode_115{[0]: ValueNode<FuncGraph> _apply_adam_114, [1]: CNode_108, [2]: beta1_power, [3]: beta2_power, [4]: CNode_109, [5]: CNode_110, [6]: lr, [7]: gradients}
#  10: @mindspore_nn_optim_adam_Adam_construct_83:CNode_117{[0]: ValueNode<Primitive> Return, [1]: CNode_116}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_87 : 000001EBEA050510
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_87 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_75]() {
  %1(CNode_57) = S_Prim_equal("mean", "mean")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %2(CNode_58) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %3(CNode_59) = Switch(%2, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_118, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_119)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %4(CNode_61) = %3()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_87:CNode_57{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<StringImm> mean, [2]: ValueNode<StringImm> mean}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_87:CNode_58{[0]: ValueNode<Primitive> Cond, [1]: CNode_57, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_87:CNode_59{[0]: ValueNode<Primitive> Switch, [1]: CNode_58, [2]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_118, [3]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_119}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_87:CNode_61{[0]: CNode_59}
#   5: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_87:CNode_62{[0]: ValueNode<Primitive> Return, [1]: CNode_61}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_91 : 000001EBEA045070
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_91 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para73_input_data) {
  %1(CNode_121) = call @mindspore_nn_layer_container_SequentialCell_construct_120(I64(0), %para73_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_91:CNode_122{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_123}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_91:CNode_121{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_120, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_91:CNode_124{[0]: ValueNode<Primitive> Return, [1]: CNode_121}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_90 : 000001EBEE5E1500
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:461/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Flatten_construct_90(%para74_x) {
  %1(x_rank) = call @rank_125(%para74_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:462/        x_rank = F.rank(x)/
  %2(CNode_126) = S_Prim_not_equal(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_127) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %4(CNode_128) = Switch(%3, @mindspore_nn_layer_basic_Flatten_construct_129, @mindspore_nn_layer_basic_Flatten_construct_130)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %5(ndim) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %6(CNode_132) = call @check_axis_valid_131(I64(1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:464/        self.check_axis_valid(self.start_dim, ndim)/
  %7(CNode_133) = call @check_axis_valid_131(I64(-1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:465/        self.check_axis_valid(self.end_dim, ndim)/
  %8(CNode_134) = MakeTuple(%6, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:461/    def construct(self, x):/
  %9(CNode_135) = StopGradient(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:461/    def construct(self, x):/
  %10(CNode_136) = S_Prim_MakeTuple(%para74_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %11(CNode_137) = S_Prim_MakeTuple("start_dim", "end_dim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %12(CNode_138) = S_Prim_MakeTuple(I64(1), I64(-1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %13(CNode_139) = S_Prim_make_dict(%11, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %14(CNode_140) = UnpackCall_unpack_call(@flatten_141, %10, %13)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %15(CNode_142) = Depend[side_effect_propagate: I64(1)](%14, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%15)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_90:x_rank{[0]: ValueNode<FuncGraph> rank_125, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Flatten_construct_90:CNode_126{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: x_rank, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Flatten_construct_90:CNode_127{[0]: ValueNode<Primitive> Cond, [1]: CNode_126, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_basic_Flatten_construct_90:CNode_128{[0]: ValueNode<Primitive> Switch, [1]: CNode_127, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_129, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_130}
#   5: @mindspore_nn_layer_basic_Flatten_construct_90:ndim{[0]: CNode_128}
#   6: @mindspore_nn_layer_basic_Flatten_construct_90:CNode_132{[0]: ValueNode<FuncGraph> check_axis_valid_131, [1]: ValueNode<Int64Imm> 1, [2]: ndim}
#   7: @mindspore_nn_layer_basic_Flatten_construct_90:CNode_133{[0]: ValueNode<FuncGraph> check_axis_valid_131, [1]: ValueNode<Int64Imm> -1, [2]: ndim}
#   8: @mindspore_nn_layer_basic_Flatten_construct_90:CNode_136{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_x}
#   9: @mindspore_nn_layer_basic_Flatten_construct_90:CNode_137{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<StringImm> start_dim, [2]: ValueNode<StringImm> end_dim}
#  10: @mindspore_nn_layer_basic_Flatten_construct_90:CNode_138{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 1, [2]: ValueNode<Int64Imm> -1}
#  11: @mindspore_nn_layer_basic_Flatten_construct_90:CNode_139{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_dict, [1]: CNode_137, [2]: CNode_138}
#  12: @mindspore_nn_layer_basic_Flatten_construct_90:CNode_140{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.143, [1]: ValueNode<FuncGraph> flatten_141, [2]: CNode_136, [3]: CNode_139}
#  13: @mindspore_nn_layer_basic_Flatten_construct_90:CNode_144{[0]: ValueNode<Primitive> Return, [1]: CNode_142}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_89 : 000001EBEE5E1FA0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:1356/    def construct(self, input):/
subgraph @mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_89(%para75_input) {
  %1(CNode_145) = S_Prim_AdaptiveAvgPool2D[output_size: (I64(1), I64(1)), input_names: ["x"], output_names: ["y"]](%para75_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/global_pool-AdaptiveAvgPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:1357/        return self.adaptive_avgpool2d(input)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/global_pool-AdaptiveAvgPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:1357/        return self.adaptive_avgpool2d(input)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_89:CNode_145{[0]: ValueNode<DoSignaturePrimitive> S_Prim_AdaptiveAvgPool2D, [1]: param_input}
#   2: @mindspore_nn_layer_pooling_AdaptiveAvgPool2d_construct_89:CNode_146{[0]: ValueNode<Primitive> Return, [1]: CNode_145}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_88 : 000001EBE7080030
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_88 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para76_input_data) {
  %1(CNode_148) = call @mindspore_nn_layer_container_SequentialCell_construct_147(I64(0), %para76_input_data)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_88:CNode_149{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: CNode_150}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_88:CNode_148{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_147, [1]: ValueNode<Int64Imm> 0, [2]: param_input_data}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_88:CNode_151{[0]: ValueNode<Primitive> Return, [1]: CNode_148}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_93 : 000001EBE7096970
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_93 parent: [subgraph @decay_weight_71]() {
  Return(%para67_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:450/        return gradients/
}
# Order:
#   1: @decay_weight_93:CNode_152{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_96 : 000001EBE7092460
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_96 parent: [subgraph @flatten_gradients_70]() {
  Return(%para68_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:427/        return gradients/
}
# Order:
#   1: @flatten_gradients_96:CNode_153{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_99 : 000001EBE7095980
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_99 parent: [subgraph @gradients_centralization_85]() {
  %1(CNode_155) = call @gradients_centralization_154()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_99:CNode_155{[0]: ValueNode<FuncGraph> gradients_centralization_154}
#   2: @gradients_centralization_99:CNode_156{[0]: ValueNode<Primitive> Return, [1]: CNode_155}


subgraph attr:
subgraph instance: assign_103 : 000001EBE707D060
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\parameter_func.py:24/def assign(variable, value):/
subgraph @assign_103(%para77_variable, %para78_value) {
  %1(CNode_157) = S_Prim_Assign[input_names: ["ref", "value"], output_names: ["output"], side_effect_mem: Bool(1)](%para77_variable, %para78_value)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\parameter_func.py:58/    return assign_(variable, value)/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\parameter_func.py:58/    return assign_(variable, value)/
}
# Order:
#   1: @assign_103:CNode_157{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Assign, [1]: param_variable, [2]: param_value}
#   2: @assign_103:CNode_158{[0]: ValueNode<Primitive> Return, [1]: CNode_157}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_114 : 000001EBE707E5A0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_114(%para79_params, %para80_beta1_power, %para81_beta2_power, %para82_moment1, %para83_moment2, %para84_lr, %para85_gradients) {
  %1(CNode_160) = call @_apply_adam_159()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:817/        if self.use_offload:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:817/        if self.use_offload:/
}
# Order:
#   1: @_apply_adam_114:CNode_160{[0]: ValueNode<FuncGraph> _apply_adam_159}
#   2: @_apply_adam_114:CNode_161{[0]: ValueNode<Primitive> Return, [1]: CNode_160}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_113 : 000001EBE7095430
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_113(%para86_gradients) {
  %1(CNode_162) = S_Prim_not_equal("CPU", "CPU")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %2(CNode_163) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %3(CNode_164) = Switch(%2, @_grad_sparse_indices_deduplicate_165, @_grad_sparse_indices_deduplicate_166)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %4(CNode_167) = %3()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %5(CNode_168) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %6(CNode_169) = Switch(%5, @_grad_sparse_indices_deduplicate_170, @_grad_sparse_indices_deduplicate_171)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %7(CNode_172) = %6()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  %8(CNode_174) = call @_grad_sparse_indices_deduplicate_173(%7)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:919/        gradients = self._grad_sparse_indices_deduplicate(gradients)/
  Return(%8)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_113:CNode_162{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: ValueNode<StringImm> CPU, [2]: ValueNode<StringImm> CPU}
#   2: @_grad_sparse_indices_deduplicate_113:CNode_163{[0]: ValueNode<Primitive> Cond, [1]: CNode_162, [2]: ValueNode<BoolImm> false}
#   3: @_grad_sparse_indices_deduplicate_113:CNode_164{[0]: ValueNode<Primitive> Switch, [1]: CNode_163, [2]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_165, [3]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_166}
#   4: @_grad_sparse_indices_deduplicate_113:CNode_167{[0]: CNode_164}
#   5: @_grad_sparse_indices_deduplicate_113:CNode_168{[0]: ValueNode<Primitive> Cond, [1]: CNode_167, [2]: ValueNode<BoolImm> false}
#   6: @_grad_sparse_indices_deduplicate_113:CNode_169{[0]: ValueNode<Primitive> Switch, [1]: CNode_168, [2]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_170, [3]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_171}
#   7: @_grad_sparse_indices_deduplicate_113:CNode_172{[0]: CNode_169}
#   8: @_grad_sparse_indices_deduplicate_113:CNode_174{[0]: ValueNode<FuncGraph> _grad_sparse_indices_deduplicate_173, [1]: CNode_172}
#   9: @_grad_sparse_indices_deduplicate_113:CNode_175{[0]: ValueNode<Primitive> Return, [1]: CNode_174}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_112 : 000001EBE7094EE0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_112(%para87_gradients) {
  %1(CNode_177) = call @scale_grad_176()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @scale_grad_112:CNode_177{[0]: ValueNode<FuncGraph> scale_grad_176}
#   2: @scale_grad_112:CNode_178{[0]: ValueNode<Primitive> Return, [1]: CNode_177}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_111 : 000001EBE7096420
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_111 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13]() {
  %1(CNode_180) = call @get_lr_179()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @get_lr_111:CNode_180{[0]: ValueNode<FuncGraph> get_lr_179}
#   2: @get_lr_111:CNode_181{[0]: ValueNode<Primitive> Return, [1]: CNode_180}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_118 : 000001EBEA04EA80
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_118 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_75]() {
  %1(x) = S_Prim_SparseSoftmaxCrossEntropyWithLogits[is_grad: Bool(0), sens: F32(1), input_names: ["features", "labels"], output_names: ["output"]](%para69_logits, %para70_labels)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:782/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:783/                return x/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_118:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SparseSoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: param_labels}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_118:CNode_63{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_119 : 000001EBEA04EFD0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_119 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_75]() {
  %1(CNode_183) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_119:CNode_183{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_119:CNode_184{[0]: ValueNode<Primitive> Return, [1]: CNode_183}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_185 : 000001EBEA04B010
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_185 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para88_x) {
  %1(CNode_187) = call @L_mindspore_nn_layer_basic_Dense_construct_186(%para88_x, %para13_classifier.0.bias, %para12_classifier.0.weight)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:classifier.0.bias>, <Ref[Tensor[Float32]], (128, 128), ref_key=:classifier.0.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_185:CNode_187{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_186, [1]: param_x, [2]: param_classifier.0.bias, [3]: param_classifier.0.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_185:CNode_188{[0]: ValueNode<Primitive> Return, [1]: CNode_187}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_189 : 000001EBEA04D540
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_189(%para89_x) {
  %1(CNode_190) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para89_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/1-ReLU)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/1-ReLU)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_189:CNode_190{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_189:CNode_191{[0]: ValueNode<Primitive> Return, [1]: CNode_190}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_192 : 000001EBEA045B10
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_192(%para90_x) {
  %1(CNode_193) = S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_194) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_195) = Switch(%2, @mindspore_nn_layer_basic_Dropout_construct_196, @mindspore_nn_layer_basic_Dropout_construct_197)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_198) = %3()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %5(CNode_199) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %6(CNode_200) = Switch(%5, @mindspore_nn_layer_basic_Dropout_construct_201, @mindspore_nn_layer_basic_Dropout_construct_202)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %7(CNode_203) = %6()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_192:CNode_193{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: ValueNode<BoolImm> true}
#   2: @mindspore_nn_layer_basic_Dropout_construct_192:CNode_194{[0]: ValueNode<Primitive> Cond, [1]: CNode_193, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_192:CNode_195{[0]: ValueNode<Primitive> Switch, [1]: CNode_194, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_196, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_197}
#   4: @mindspore_nn_layer_basic_Dropout_construct_192:CNode_198{[0]: CNode_195}
#   5: @mindspore_nn_layer_basic_Dropout_construct_192:CNode_199{[0]: ValueNode<Primitive> Cond, [1]: CNode_198, [2]: ValueNode<BoolImm> false}
#   6: @mindspore_nn_layer_basic_Dropout_construct_192:CNode_200{[0]: ValueNode<Primitive> Switch, [1]: CNode_199, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_201, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_202}
#   7: @mindspore_nn_layer_basic_Dropout_construct_192:CNode_203{[0]: CNode_200}
#   8: @mindspore_nn_layer_basic_Dropout_construct_192:CNode_204{[0]: ValueNode<Primitive> Return, [1]: CNode_203}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_205 : 000001EBEA0475A0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_205 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para91_x) {
  %1(CNode_206) = call @L_mindspore_nn_layer_basic_Dense_construct_186(%para91_x, %para15_classifier.3.bias, %para14_classifier.3.weight)
      : (<null>, <Ref[Tensor[Float32]], (7), ref_key=:classifier.3.bias>, <Ref[Tensor[Float32]], (7, 128), ref_key=:classifier.3.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/3-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_205:CNode_206{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_186, [1]: param_x, [2]: param_classifier.3.bias, [3]: param_classifier.3.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_205:CNode_207{[0]: ValueNode<Primitive> Return, [1]: CNode_206}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_120 : 000001EBEA04B560
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_120 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_91](%para92_, %para93_) {
  %1(CNode_123) = $(mindspore_nn_layer_container_SequentialCell_construct_91):MakeTuple(@mindspore_nn_layer_basic_Dense_construct_185, @mindspore_nn_layer_activation_ReLU_construct_189, @mindspore_nn_layer_basic_Dropout_construct_192, @mindspore_nn_layer_basic_Dense_construct_205)
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_122) = $(mindspore_nn_layer_container_SequentialCell_construct_91):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_208) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para92_@CNode_209, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_210) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_211, @mindspore_nn_layer_container_SequentialCell_construct_212)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_213) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_120:CNode_208{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.9, [1]: param_@CNode_209, [2]: CNode_122}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_120:CNode_210{[0]: ValueNode<Primitive> Switch, [1]: CNode_208, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_211, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_212}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_120:CNode_213{[0]: CNode_210}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_120:CNode_214{[0]: ValueNode<Primitive> Return, [1]: CNode_213}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_131 : 000001EBEE5DFFC0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_131(%para94_axis, %para95_ndim) {
  %1(CNode_215) = S_Prim_negative(%para95_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_216) = S_Prim_less(%para94_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %3(CNode_217) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %4(CNode_218) = Switch(%3, @check_axis_valid_219, @check_axis_valid_220)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %5(CNode_221) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %6(CNode_222) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %7(CNode_223) = Switch(%6, @check_axis_valid_224, @check_axis_valid_225)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %8(CNode_226) = %7()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_131:CNode_215{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_131:CNode_216{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_215}
#   3: @check_axis_valid_131:CNode_217{[0]: ValueNode<Primitive> Cond, [1]: CNode_216, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_131:CNode_218{[0]: ValueNode<Primitive> Switch, [1]: CNode_217, [2]: ValueNode<FuncGraph> check_axis_valid_219, [3]: ValueNode<FuncGraph> check_axis_valid_220}
#   5: @check_axis_valid_131:CNode_221{[0]: CNode_218}
#   6: @check_axis_valid_131:CNode_222{[0]: ValueNode<Primitive> Cond, [1]: CNode_221, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_131:CNode_223{[0]: ValueNode<Primitive> Switch, [1]: CNode_222, [2]: ValueNode<FuncGraph> check_axis_valid_224, [3]: ValueNode<FuncGraph> check_axis_valid_225}
#   8: @check_axis_valid_131:CNode_226{[0]: CNode_223}
#   9: @check_axis_valid_131:CNode_227{[0]: ValueNode<Primitive> Return, [1]: CNode_226}


subgraph attr:
subgraph instance: rank_125 : 000001EBEE5F3E80
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1541/def rank(input_x):/
subgraph @rank_125(%para96_input_x) {
  %1(CNode_228) = S_Prim_Rank(%para96_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1571/    return rank_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1571/    return rank_(input_x)/
}
# Order:
#   1: @rank_125:CNode_228{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input_x}
#   2: @rank_125:CNode_229{[0]: ValueNode<Primitive> Return, [1]: CNode_228}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_129 : 000001EBEA044080
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @mindspore_nn_layer_basic_Flatten_construct_129 parent: [subgraph @mindspore_nn_layer_basic_Flatten_construct_90]() {
  %1(x_rank) = $(mindspore_nn_layer_basic_Flatten_construct_90):call @rank_125(%para74_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:462/        x_rank = F.rank(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_129:CNode_230{[0]: ValueNode<Primitive> Return, [1]: x_rank}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_130 : 000001EBEA0465B0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @mindspore_nn_layer_basic_Flatten_construct_130() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_130:CNode_231{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: flatten_141 : 000001EBEE5E3A30
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_141(%para97_input, %para98_order, %para99_start_dim, %para100_end_dim) {
  %1(CNode_232) = S_Prim_isinstance(%para97_input, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  %2(CNode_233) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  %3(CNode_234) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  %4(CNode_235) = Switch(%3, @flatten_236, @flatten_237)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  %5(CNode_238) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @flatten_141:CNode_232{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_input, [2]: ValueNode<ClassType> class 'mindspore.common.tensor.Tensor'}
#   2: @flatten_141:CNode_233{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_232}
#   3: @flatten_141:CNode_234{[0]: ValueNode<Primitive> Cond, [1]: CNode_233, [2]: ValueNode<BoolImm> false}
#   4: @flatten_141:CNode_235{[0]: ValueNode<Primitive> Switch, [1]: CNode_234, [2]: ValueNode<FuncGraph> flatten_236, [3]: ValueNode<FuncGraph> flatten_237}
#   5: @flatten_141:CNode_238{[0]: CNode_235}
#   6: @flatten_141:CNode_239{[0]: ValueNode<Primitive> Return, [1]: CNode_238}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_240 : 000001EBEE5E0A60
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_240 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para101_x) {
  %1(CNode_242) = call @mindspore_nn_layer_conv_Conv2d_construct_241()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/0-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/0-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_240:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.0.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_240:CNode_242{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_241}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_240:CNode_243{[0]: ValueNode<Primitive> Return, [1]: CNode_242}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_244 : 000001EBEE5DB560
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_244 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para102_x) {
  %1(CNode_245) = S_Prim_Shape(%para102_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_246) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_247) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_248) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_249) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_250) = Switch(%5, @mindspore_nn_layer_normalization_BatchNorm2d_construct_251, @mindspore_nn_layer_normalization_BatchNorm2d_construct_252)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_253) = %6()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_254) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_244:CNode_245{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_244:CNode_246{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_245, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_244:CNode_248{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_244:CNode_249{[0]: ValueNode<Primitive> Cond, [1]: CNode_248, [2]: ValueNode<BoolImm> false}
#   5: @mindspore_nn_layer_normalization_BatchNorm2d_construct_244:CNode_250{[0]: ValueNode<Primitive> Switch, [1]: CNode_249, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_251, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_252}
#   6: @mindspore_nn_layer_normalization_BatchNorm2d_construct_244:CNode_253{[0]: CNode_250}
#   7: @mindspore_nn_layer_normalization_BatchNorm2d_construct_244:CNode_255{[0]: ValueNode<Primitive> Return, [1]: CNode_254}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_256 : 000001EBEE5DB010
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_256(%para103_x) {
  %1(CNode_257) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para103_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/2-ReLU)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/2-ReLU)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_256:CNode_257{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_256:CNode_258{[0]: ValueNode<Primitive> Return, [1]: CNode_257}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_259 : 000001EBEE5D8590
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_259(%para104_x) {
  %1(CNode_260) = getattr(%para104_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %2(CNode_261) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %3(CNode_262) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %4(CNode_263) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_264, @mindspore_nn_layer_pooling_MaxPool2d_construct_265)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %5(CNode_266) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_259:CNode_260{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_259:CNode_261{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_260, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_259:CNode_262{[0]: ValueNode<Primitive> Cond, [1]: CNode_261, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_259:CNode_263{[0]: ValueNode<Primitive> Switch, [1]: CNode_262, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_264, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_265}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_259:CNode_266{[0]: CNode_263}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_259:CNode_267{[0]: ValueNode<Primitive> Return, [1]: CNode_266}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_268 : 000001EBEE5D75A0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_268 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para105_x) {
  %1(CNode_270) = call @mindspore_nn_layer_conv_Conv2d_construct_269()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/4-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/4-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_268:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.4.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_268:CNode_270{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_269}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_268:CNode_271{[0]: ValueNode<Primitive> Return, [1]: CNode_270}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_272 : 000001EBE708BF70
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_272 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para106_x) {
  %1(CNode_273) = S_Prim_Shape(%para106_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_274) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_275) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_276) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_277) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_278) = Switch(%5, @mindspore_nn_layer_normalization_BatchNorm2d_construct_279, @mindspore_nn_layer_normalization_BatchNorm2d_construct_280)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_281) = %6()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_282) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_272:CNode_273{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_272:CNode_274{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_273, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_272:CNode_276{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_272:CNode_277{[0]: ValueNode<Primitive> Cond, [1]: CNode_276, [2]: ValueNode<BoolImm> false}
#   5: @mindspore_nn_layer_normalization_BatchNorm2d_construct_272:CNode_278{[0]: ValueNode<Primitive> Switch, [1]: CNode_277, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_279, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_280}
#   6: @mindspore_nn_layer_normalization_BatchNorm2d_construct_272:CNode_281{[0]: CNode_278}
#   7: @mindspore_nn_layer_normalization_BatchNorm2d_construct_272:CNode_283{[0]: ValueNode<Primitive> Return, [1]: CNode_282}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_284 : 000001EBE708E9F0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_284(%para107_x) {
  %1(CNode_285) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para107_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/6-ReLU)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/6-ReLU)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_284:CNode_285{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_284:CNode_286{[0]: ValueNode<Primitive> Return, [1]: CNode_285}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_287 : 000001EBE7089A40
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_287(%para108_x) {
  %1(CNode_288) = getattr(%para108_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %2(CNode_289) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %3(CNode_290) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %4(CNode_291) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_292, @mindspore_nn_layer_pooling_MaxPool2d_construct_293)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %5(CNode_294) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_287:CNode_288{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_287:CNode_289{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_288, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_287:CNode_290{[0]: ValueNode<Primitive> Cond, [1]: CNode_289, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_287:CNode_291{[0]: ValueNode<Primitive> Switch, [1]: CNode_290, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_292, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_293}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_287:CNode_294{[0]: CNode_291}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_287:CNode_295{[0]: ValueNode<Primitive> Return, [1]: CNode_294}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_296 : 000001EBE7087A60
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_296 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para109_x) {
  %1(CNode_298) = call @mindspore_nn_layer_conv_Conv2d_construct_297()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/8-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/8-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_296:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_features.8.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_296:CNode_298{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_297}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_296:CNode_299{[0]: ValueNode<Primitive> Return, [1]: CNode_298}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_300 : 000001EBE7087510
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_300 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13](%para110_x) {
  %1(CNode_301) = S_Prim_Shape(%para110_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %2(CNode_302) = S_Prim__check_input_dim[constexpr_prim: Bool(1)](%1, "BatchNorm2d")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:139/        self._check_input_dim(self.shape(x), self.cls_name)/
  %3(CNode_303) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
  %4(CNode_304) = S_Prim_is_(None, None)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %5(CNode_305) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %6(CNode_306) = Switch(%5, @mindspore_nn_layer_normalization_BatchNorm2d_construct_307, @mindspore_nn_layer_normalization_BatchNorm2d_construct_308)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %7(CNode_309) = %6()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  %8(CNode_310) = Depend[side_effect_propagate: I64(1)](%7, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_300:CNode_301{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_300:CNode_302{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_input_dim, [1]: CNode_301, [2]: ValueNode<StringImm> BatchNorm2d}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_300:CNode_304{[0]: ValueNode<DoSignaturePrimitive> S_Prim_is_, [1]: ValueNode<None> None, [2]: ValueNode<None> None}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_300:CNode_305{[0]: ValueNode<Primitive> Cond, [1]: CNode_304, [2]: ValueNode<BoolImm> false}
#   5: @mindspore_nn_layer_normalization_BatchNorm2d_construct_300:CNode_306{[0]: ValueNode<Primitive> Switch, [1]: CNode_305, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_307, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_308}
#   6: @mindspore_nn_layer_normalization_BatchNorm2d_construct_300:CNode_309{[0]: CNode_306}
#   7: @mindspore_nn_layer_normalization_BatchNorm2d_construct_300:CNode_311{[0]: ValueNode<Primitive> Return, [1]: CNode_310}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_312 : 000001EBE7081020
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_312(%para111_x) {
  %1(CNode_313) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para111_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/10-ReLU)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/10-ReLU)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_312:CNode_313{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_312:CNode_314{[0]: ValueNode<Primitive> Return, [1]: CNode_313}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_315 : 000001EBE707C5C0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_315(%para112_x) {
  %1(CNode_316) = getattr(%para112_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %2(CNode_317) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %3(CNode_318) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %4(CNode_319) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_320, @mindspore_nn_layer_pooling_MaxPool2d_construct_321)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %5(CNode_322) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_315:CNode_316{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_315:CNode_317{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_316, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_315:CNode_318{[0]: ValueNode<Primitive> Cond, [1]: CNode_317, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_315:CNode_319{[0]: ValueNode<Primitive> Switch, [1]: CNode_318, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_320, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_321}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_315:CNode_322{[0]: CNode_319}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_315:CNode_323{[0]: ValueNode<Primitive> Return, [1]: CNode_322}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_147 : 000001EBEE5DF520
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_147 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_88](%para113_, %para114_) {
  %1(CNode_150) = $(mindspore_nn_layer_container_SequentialCell_construct_88):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_240, @mindspore_nn_layer_normalization_BatchNorm2d_construct_244, @mindspore_nn_layer_activation_ReLU_construct_256, @mindspore_nn_layer_pooling_MaxPool2d_construct_259, @mindspore_nn_layer_conv_Conv2d_construct_268, @mindspore_nn_layer_normalization_BatchNorm2d_construct_272, @mindspore_nn_layer_activation_ReLU_construct_284, @mindspore_nn_layer_pooling_MaxPool2d_construct_287, @mindspore_nn_layer_conv_Conv2d_construct_296, @mindspore_nn_layer_normalization_BatchNorm2d_construct_300, @mindspore_nn_layer_activation_ReLU_construct_312, @mindspore_nn_layer_pooling_MaxPool2d_construct_315)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_149) = $(mindspore_nn_layer_container_SequentialCell_construct_88):S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %3(CNode_324) = MultitypeFuncGraph_less{(Number, Number), (String, String), (Tuple, Tuple), (Tensor, Tensor), (Number, Tensor), (Tensor, Number), (List, List)}(%para113_@CNode_325, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_326) = Switch(%3, @mindspore_nn_layer_container_SequentialCell_construct_327, @mindspore_nn_layer_container_SequentialCell_construct_328)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(CNode_329) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_147:CNode_324{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-less.9, [1]: param_@CNode_325, [2]: CNode_149}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_147:CNode_326{[0]: ValueNode<Primitive> Switch, [1]: CNode_324, [2]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_327, [3]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_328}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_147:CNode_329{[0]: CNode_326}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_147:CNode_330{[0]: ValueNode<Primitive> Return, [1]: CNode_329}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_154 : 000001EBE7098950
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_154 parent: [subgraph @gradients_centralization_85]() {
  Return(%para72_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:469/        return gradients/
}
# Order:
#   1: @gradients_centralization_154:CNode_331{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_159 : 000001EBE707E050
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_159 parent: [subgraph @_apply_adam_114]() {
  %1(CNode_333) = call @_apply_adam_332()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:826/            if self.use_dist_optimizer:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:826/            if self.use_dist_optimizer:/
}
# Order:
#   1: @_apply_adam_159:CNode_333{[0]: ValueNode<FuncGraph> _apply_adam_332}
#   2: @_apply_adam_159:CNode_334{[0]: ValueNode<Primitive> Return, [1]: CNode_333}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
after_block : 1
subgraph instance: _grad_sparse_indices_deduplicate_173 : 000001EBE7099E90
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_173(%para115_) {
  Return(%para115_phi_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:521/        return gradients/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_173:CNode_335{[0]: ValueNode<Primitive> Return, [1]: param_phi_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_170 : 000001EBE7097960
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_170 parent: [subgraph @_grad_sparse_indices_deduplicate_113]() {
  %1(CNode_336) = S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_indices_deduplicate)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:520/            gradients = self.map_(F.partial(_indices_deduplicate), gradients)/
  %2(gradients) = S_Prim_map(%1, %para86_gradients)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:520/            gradients = self.map_(F.partial(_indices_deduplicate), gradients)/
  Return(%2)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:520/            gradients = self.map_(F.partial(_indices_deduplicate), gradients)/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_170:CNode_336{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_indices_deduplicate}
#   2: @_grad_sparse_indices_deduplicate_170:gradients{[0]: ValueNode<DoSignaturePrimitive> S_Prim_map, [1]: CNode_336, [2]: param_gradients}
#   3: @_grad_sparse_indices_deduplicate_170:CNode_337{[0]: ValueNode<Primitive> Return, [1]: gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_171 : 000001EBE7099940
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_171 parent: [subgraph @_grad_sparse_indices_deduplicate_113]() {
  Return(%para86_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_171:CNode_338{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_165 : 000001EBE70993F0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_165() {
  Return(Bool(1))
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_165:CNode_339{[0]: ValueNode<Primitive> Return, [1]: ValueNode<BoolImm> true}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _grad_sparse_indices_deduplicate_166 : 000001EBE7097EB0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:517/    def _grad_sparse_indices_deduplicate(self, gradients):/
subgraph @_grad_sparse_indices_deduplicate_166 parent: [subgraph @_grad_sparse_indices_deduplicate_113]() {
  %1(CNode_162) = $(_grad_sparse_indices_deduplicate_113):S_Prim_not_equal("CPU", "CPU")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:519/        if self._target != 'CPU' and self._unique:/
}
# Order:
#   1: @_grad_sparse_indices_deduplicate_166:CNode_340{[0]: ValueNode<Primitive> Return, [1]: CNode_162}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_176 : 000001EBE7098EA0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_176 parent: [subgraph @scale_grad_112]() {
  %1(CNode_342) = call @scale_grad_341()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @scale_grad_176:CNode_342{[0]: ValueNode<FuncGraph> scale_grad_341}
#   2: @scale_grad_176:CNode_343{[0]: ValueNode<Primitive> Return, [1]: CNode_342}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_179 : 000001EBE709A3E0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_179 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13]() {
  %1(CNode_345) = call @get_lr_344()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @get_lr_179:CNode_345{[0]: ValueNode<FuncGraph> get_lr_344}
#   2: @get_lr_179:CNode_346{[0]: ValueNode<Primitive> Return, [1]: CNode_345}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182 : 000001EBEA050A60
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_75]() {
  %1(CNode_348) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_347()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182:CNode_349{[0]: ValueNode<FuncGraph> shape_350, [1]: param_logits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182:CNode_351{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182:CNode_352{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_349, [2]: CNode_351}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182:labels{[0]: ValueNode<DoSignaturePrimitive> S_Prim_OneHot, [1]: param_labels, [2]: CNode_352, [3]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=1), [4]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0)}
#   5: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182:CNode_348{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_347}
#   6: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182:CNode_353{[0]: ValueNode<Primitive> Return, [1]: CNode_348}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_186 : 000001EBEA044B20
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_186(%para116_x, %para117_, %para118_) {
  %1(x_shape) = S_Prim_Shape(%para116_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_354) = S_Prim_check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:624/        check_dense_input_shape(x_shape, self.cls_name)/
  %3(CNode_355) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
  %4(CNode_356) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %5(CNode_357) = S_Prim_not_equal(%4, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %6(CNode_358) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %7(CNode_359) = Switch(%6, @L_mindspore_nn_layer_basic_Dense_construct_360, @L_mindspore_nn_layer_basic_Dense_construct_361)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %8(CNode_362) = %7()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %9(CNode_364) = call @L_mindspore_nn_layer_basic_Dense_construct_363(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %10(CNode_365) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_186:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_186:CNode_354{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_186:CNode_356{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_186:CNode_357{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_356, [2]: ValueNode<Int64Imm> 2}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_186:CNode_358{[0]: ValueNode<Primitive> Cond, [1]: CNode_357, [2]: ValueNode<BoolImm> false}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_186:CNode_359{[0]: ValueNode<Primitive> Switch, [1]: CNode_358, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_360, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_361}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_186:CNode_362{[0]: CNode_359}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_186:CNode_364{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_363, [1]: CNode_362}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_186:CNode_365{[0]: ValueNode<Primitive> Depend, [1]: CNode_364, [2]: CNode_355}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_186:CNode_188{[0]: ValueNode<Primitive> Return, [1]: CNode_365}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_201 : 000001EBEA04A570
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_201 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_192]() {
  Return(%para90_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:192/            return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_201:CNode_366{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_202 : 000001EBEA04C550
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_202 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_192]() {
  %1(CNode_368) = call @mindspore_nn_layer_basic_Dropout_construct_367()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_202:CNode_368{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_367}
#   2: @mindspore_nn_layer_basic_Dropout_construct_202:CNode_369{[0]: ValueNode<Primitive> Return, [1]: CNode_368}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_196 : 000001EBEA04CAA0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_196 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_192]() {
  %1(CNode_193) = $(mindspore_nn_layer_basic_Dropout_construct_192):S_Prim_logical_not(Bool(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_196:CNode_370{[0]: ValueNode<Primitive> Return, [1]: CNode_193}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_197 : 000001EBEA04AAC0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_197() {
  %1(CNode_371) = S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %2(CNode_372) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %3(CNode_373) = Switch(%2, @mindspore_nn_layer_basic_Dropout_construct_374, @mindspore_nn_layer_basic_Dropout_construct_375)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  %4(CNode_376) = %3()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_197:CNode_371{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<FP32Imm> 0.5, [2]: ValueNode<Int64Imm> 1}
#   2: @mindspore_nn_layer_basic_Dropout_construct_197:CNode_372{[0]: ValueNode<Primitive> Cond, [1]: CNode_371, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_basic_Dropout_construct_197:CNode_373{[0]: ValueNode<Primitive> Switch, [1]: CNode_372, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_374, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dropout_construct_375}
#   4: @mindspore_nn_layer_basic_Dropout_construct_197:CNode_376{[0]: CNode_373}
#   5: @mindspore_nn_layer_basic_Dropout_construct_197:CNode_377{[0]: ValueNode<Primitive> Return, [1]: CNode_376}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_211 : 000001EBEA04DA90
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_211 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_120]() {
  %1(CNode_209) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para92_@CNode_209, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_378) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_123) = $(mindspore_nn_layer_container_SequentialCell_construct_91):MakeTuple(@mindspore_nn_layer_basic_Dense_construct_185, @mindspore_nn_layer_activation_ReLU_construct_189, @mindspore_nn_layer_basic_Dropout_construct_192, @mindspore_nn_layer_basic_Dense_construct_205)
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_380) = call @ms_iter_379(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para92_@CNode_209)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para93_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_381) = call @mindspore_nn_layer_container_SequentialCell_construct_120(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_382) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_211:CNode_380{[0]: ValueNode<FuncGraph> ms_iter_379, [1]: CNode_123}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_211:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_380, [2]: param_@CNode_209}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_211:CNode_209{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.383, [1]: param_@CNode_209, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_211:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_211:CNode_381{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_120, [1]: CNode_209, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_211:CNode_384{[0]: ValueNode<Primitive> Return, [1]: CNode_382}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_212 : 000001EBEA049AD0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_212 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_120]() {
  Return(%para93_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_212:CNode_385{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_224 : 000001EBEE5DEA80
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_224() {
  %1(CNode_386) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @check_axis_valid_224:CNode_386{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @check_axis_valid_224:CNode_387{[0]: ValueNode<Primitive> Return, [1]: CNode_386}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_225 : 000001EBEE5DE530
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_225() {
  %1(CNode_389) = call @check_axis_valid_388()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_225:CNode_389{[0]: ValueNode<FuncGraph> check_axis_valid_388}
#   2: @check_axis_valid_225:CNode_390{[0]: ValueNode<Primitive> Return, [1]: CNode_389}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_219 : 000001EBEE5E1A50
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_219 parent: [subgraph @check_axis_valid_131]() {
  %1(CNode_215) = $(check_axis_valid_131):S_Prim_negative(%para95_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_216) = $(check_axis_valid_131):S_Prim_less(%para94_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_219:CNode_391{[0]: ValueNode<Primitive> Return, [1]: CNode_216}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_220 : 000001EBEE5E0510
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_220 parent: [subgraph @check_axis_valid_131]() {
  %1(CNode_392) = S_Prim_greater_equal(%para94_axis, %para95_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_220:CNode_392{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @check_axis_valid_220:CNode_393{[0]: ValueNode<Primitive> Return, [1]: CNode_392}


subgraph attr:
subgraph instance: flatten_236 : 000001EBEE5F43D0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_236() {
  %1(CNode_394) = JoinedStr("For 'flatten', argument 'input' must be Tensor.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  %2(CNode_395) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
}
# Order:
#   1: @flatten_236:CNode_394{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', argument 'input' must be Tensor.}
#   2: @flatten_236:CNode_395{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_394, [3]: ValueNode<StringImm> None}
#   3: @flatten_236:CNode_396{[0]: ValueNode<Primitive> Return, [1]: CNode_395}


subgraph attr:
subgraph instance: flatten_237 : 000001EBEE5E4F70
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_237 parent: [subgraph @flatten_141]() {
  %1(CNode_398) = call @flatten_397()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @flatten_237:CNode_398{[0]: ValueNode<FuncGraph> flatten_397}
#   2: @flatten_237:CNode_399{[0]: ValueNode<Primitive> Return, [1]: CNode_398}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_241 : 000001EBEE5DEFD0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_241 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_240]() {
  %1(CNode_401) = call @mindspore_nn_layer_conv_Conv2d_construct_400()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/0-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/0-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_241:CNode_401{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_400}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_241:CNode_402{[0]: ValueNode<Primitive> Return, [1]: CNode_401}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_251 : 000001EBEE5D9580
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_251 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_244]() {
  %1(CNode_404) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_403()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_251:CNode_404{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_403}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_251:CNode_405{[0]: ValueNode<Primitive> Return, [1]: CNode_404}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_252 : 000001EBEE5DBAB0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_252 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_244]() {
  %1(CNode_407) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_406()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_252:CNode_407{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_406}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_252:CNode_408{[0]: ValueNode<Primitive> Return, [1]: CNode_407}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_264 : 000001EBEE5DCAA0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_264 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_259]() {
  %1(CNode_409) = getattr(%para104_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_411) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_410(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_264:CNode_409{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_264:x{[0]: CNode_409, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_264:CNode_412{[0]: ValueNode<Primitive> Return, [1]: CNode_411}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_264:CNode_411{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_410, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_265 : 000001EBEE5D7AF0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_265 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_259]() {
  %1(CNode_413) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_410(%para104_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_265:CNode_414{[0]: ValueNode<Primitive> Return, [1]: CNode_413}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_265:CNode_413{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_410, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_269 : 000001EBEE5D7050
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_269 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_268]() {
  %1(CNode_416) = call @mindspore_nn_layer_conv_Conv2d_construct_415()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/4-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/4-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_269:CNode_416{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_415}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_269:CNode_417{[0]: ValueNode<Primitive> Return, [1]: CNode_416}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_279 : 000001EBE708D4B0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_279 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_272]() {
  %1(CNode_419) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_418()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_279:CNode_419{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_418}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_279:CNode_420{[0]: ValueNode<Primitive> Return, [1]: CNode_419}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_280 : 000001EBE708FF30
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_280 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_272]() {
  %1(CNode_422) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_421()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_280:CNode_422{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_421}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_280:CNode_423{[0]: ValueNode<Primitive> Return, [1]: CNode_422}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_292 : 000001EBE708E4A0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_292 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_287]() {
  %1(CNode_424) = getattr(%para108_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_426) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_425(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_292:CNode_424{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_292:x{[0]: CNode_424, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_292:CNode_427{[0]: ValueNode<Primitive> Return, [1]: CNode_426}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_292:CNode_426{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_425, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_293 : 000001EBE708A4E0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_293 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_287]() {
  %1(CNode_428) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_425(%para108_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_293:CNode_429{[0]: ValueNode<Primitive> Return, [1]: CNode_428}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_293:CNode_428{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_425, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_297 : 000001EBE7085A80
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_297 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_296]() {
  %1(CNode_431) = call @mindspore_nn_layer_conv_Conv2d_construct_430()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/8-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/8-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_297:CNode_431{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_430}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_297:CNode_432{[0]: ValueNode<Primitive> Return, [1]: CNode_431}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_307 : 000001EBE7086FC0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_307 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_300]() {
  %1(CNode_434) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_433()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:141/            if self.training:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_307:CNode_434{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_433}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_307:CNode_435{[0]: ValueNode<Primitive> Return, [1]: CNode_434}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_308 : 000001EBE7084FE0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_308 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_300]() {
  %1(CNode_437) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_436()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:140/        if self.use_batch_statistics is None:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_308:CNode_437{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_436}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_308:CNode_438{[0]: ValueNode<Primitive> Return, [1]: CNode_437}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_320 : 000001EBE7083550
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_320 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_315]() {
  %1(CNode_439) = getattr(%para112_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_441) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_440(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_320:CNode_439{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_320:x{[0]: CNode_439, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_320:CNode_442{[0]: ValueNode<Primitive> Return, [1]: CNode_441}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_320:CNode_441{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_440, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_321 : 000001EBE7081570
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_321 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_315]() {
  %1(CNode_443) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_440(%para112_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_321:CNode_444{[0]: ValueNode<Primitive> Return, [1]: CNode_443}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_321:CNode_443{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_440, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_327 : 000001EBEE5E0FB0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_327 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_147]() {
  %1(CNode_325) = MultitypeFuncGraph_add{(Number, Number), (String, String), (Tensor, Tuple), (Number, Tensor), (Tensor, Number), (List, Tensor), (Tuple, Tensor), (NoneType, NoneType), (Tensor, List), (List, List), (Tensor, Tensor), (RowTensor, Tensor), (COOTensor, COOTensor), (Tuple, Tuple), (CSRTensor, CSRTensor), (COOTensor, Tensor), (Tensor, COOTensor), (Dictionary, Dictionary)}(%para113_@CNode_325, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %2(CNode_445) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
  %3(CNode_150) = $(mindspore_nn_layer_container_SequentialCell_construct_88):MakeTuple(@mindspore_nn_layer_conv_Conv2d_construct_240, @mindspore_nn_layer_normalization_BatchNorm2d_construct_244, @mindspore_nn_layer_activation_ReLU_construct_256, @mindspore_nn_layer_pooling_MaxPool2d_construct_259, @mindspore_nn_layer_conv_Conv2d_construct_268, @mindspore_nn_layer_normalization_BatchNorm2d_construct_272, @mindspore_nn_layer_activation_ReLU_construct_284, @mindspore_nn_layer_pooling_MaxPool2d_construct_287, @mindspore_nn_layer_conv_Conv2d_construct_296, @mindspore_nn_layer_normalization_BatchNorm2d_construct_300, @mindspore_nn_layer_activation_ReLU_construct_312, @mindspore_nn_layer_pooling_MaxPool2d_construct_315)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %4(CNode_446) = call @ms_iter_379(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %5(cell) = S_Prim_getitem(%4, %para113_@CNode_325)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %6(input_data) = %5(%para114_phi_input_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:295/            input_data = cell(input_data)/
  %7(CNode_447) = call @mindspore_nn_layer_container_SequentialCell_construct_147(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  %8(CNode_448) = Depend[side_effect_propagate: I64(1)](%7, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_327:CNode_446{[0]: ValueNode<FuncGraph> ms_iter_379, [1]: CNode_150}
#   2: @mindspore_nn_layer_container_SequentialCell_construct_327:cell{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_446, [2]: param_@CNode_325}
#   3: @mindspore_nn_layer_container_SequentialCell_construct_327:CNode_325{[0]: ValueNode<MultitypeFuncGraph> MetaFuncGraph-add.383, [1]: param_@CNode_325, [2]: ValueNode<Int64Imm> 1}
#   4: @mindspore_nn_layer_container_SequentialCell_construct_327:input_data{[0]: cell, [1]: param_phi_input_data}
#   5: @mindspore_nn_layer_container_SequentialCell_construct_327:CNode_447{[0]: ValueNode<FuncGraph> mindspore_nn_layer_container_SequentialCell_construct_147, [1]: CNode_325, [2]: input_data}
#   6: @mindspore_nn_layer_container_SequentialCell_construct_327:CNode_449{[0]: ValueNode<Primitive> Return, [1]: CNode_448}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_container_SequentialCell_construct_328 : 000001EBEE5DFA70
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:293/    def construct(self, input_data):/
subgraph @mindspore_nn_layer_container_SequentialCell_construct_328 parent: [subgraph @mindspore_nn_layer_container_SequentialCell_construct_147]() {
  Return(%para114_phi_input_data)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:296/        return input_data/
}
# Order:
#   1: @mindspore_nn_layer_container_SequentialCell_construct_328:CNode_450{[0]: ValueNode<Primitive> Return, [1]: param_phi_input_data}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_332 : 000001EBE707CB10
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_332 parent: [subgraph @_apply_adam_114]() {
  %1(CNode_452) = call @_apply_adam_451()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:866/                if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:866/                if self.is_group_lr:/
}
# Order:
#   1: @_apply_adam_332:CNode_452{[0]: ValueNode<FuncGraph> _apply_adam_451}
#   2: @_apply_adam_332:CNode_453{[0]: ValueNode<Primitive> Return, [1]: CNode_452}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_341 : 000001EBE7096EC0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_341 parent: [subgraph @scale_grad_112]() {
  Return(%para87_gradients)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:488/        return gradients/
}
# Order:
#   1: @scale_grad_341:CNode_454{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_344 : 000001EBE709A930
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_344 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_13]() {
  Return(%para45_learning_rate)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\optimizer.py:756/        return lr/
}
# Order:
#   1: @get_lr_344:CNode_455{[0]: ValueNode<Primitive> Return, [1]: param_learning_rate}


subgraph attr:
subgraph instance: shape_350 : 000001EBEA050FB0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1484/def shape(input_x):/
subgraph @shape_350(%para119_input_x) {
  %1(CNode_456) = S_Prim_Shape(%para119_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1510/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1510/    return shape_(input_x)/
}
# Order:
#   1: @shape_350:CNode_456{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @shape_350:CNode_457{[0]: ValueNode<Primitive> Return, [1]: CNode_456}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_347 : 000001EBEA051500
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_347 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182]() {
  %1(CNode_349) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182):call @shape_350(%para69_logits)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %2(CNode_351) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182):S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %3(CNode_352) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182):S_Prim_getitem(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %4(labels) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_182):S_Prim_OneHot[axis: I64(-1), input_names: ["indices", "depth", "on_value", "off_value"], output_names: ["output"]](%para70_labels, %3, Tensor(shape=[], dtype=Float32, value=1), Tensor(shape=[], dtype=Float32, value=0))
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %5(CNode_458) = S_Prim_SoftmaxCrossEntropyWithLogits(%para69_logits, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %6(x) = S_Prim_getitem(%5, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %7(CNode_460) = call @get_loss_459(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:786/        return self.get_loss(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:786/        return self.get_loss(x)/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_347:CNode_458{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: labels}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_347:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_458, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_347:CNode_460{[0]: ValueNode<FuncGraph> get_loss_459, [1]: x}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_347:CNode_461{[0]: ValueNode<Primitive> Return, [1]: CNode_460}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_363 : 000001EBEA047AF0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_363 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_186](%para120_) {
  %1(CNode_463) = call @L_mindspore_nn_layer_basic_Dense_construct_462()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:628/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:628/        if self.has_bias:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_363:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MatMul, [1]: param_phi_x, [2]: param_L_classifier.0.weight}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_363:CNode_463{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_462}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_363:CNode_464{[0]: ValueNode<Primitive> Return, [1]: CNode_463}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_360 : 000001EBEA046B00
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_360 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_186]() {
  %1(CNode_465) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %2(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_186):S_Prim_Shape(%para116_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %3(CNode_466) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %4(CNode_467) = S_Prim_getitem(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %5(CNode_468) = S_Prim_MakeTuple(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %6(x) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para116_x, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_360:CNode_465{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_360:CNode_466{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_360:CNode_467{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_466}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_360:CNode_468{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_465, [2]: CNode_467}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_360:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_x, [2]: CNode_468}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_360:CNode_469{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_361 : 000001EBEA047050
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_361 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_186]() {
  Return(%para116_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_361:CNode_470{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_367 : 000001EBEA04CFF0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_367 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_192]() {
  %1(CNode_471) = S_Prim_Dropout[keep_prob: F32(0.5), Seed0: I64(0), Seed1: I64(0), side_effect_hidden: Bool(1)](%para90_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:194/        out, _ = self.dropout(x)/
  %2(out) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:194/        out, _ = self.dropout(x)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:195/        return out/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_367:CNode_471{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Dropout, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Dropout_construct_367:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_471, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Dropout_construct_367:CNode_472{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_374 : 000001EBEA04C000
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_374 parent: [subgraph @mindspore_nn_layer_basic_Dropout_construct_197]() {
  %1(CNode_371) = $(mindspore_nn_layer_basic_Dropout_construct_197):S_Prim_equal(F32(0.5), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_374:CNode_473{[0]: ValueNode<Primitive> Return, [1]: CNode_371}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dropout_construct_375 : 000001EBEA04BAB0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:190/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dropout_construct_375() {
  %1(CNode_474) = S_Prim_equal(None, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/2-Dropout)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:191/        if not self.training or self.keep_prob == 1 or self.p == 0:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dropout_construct_375:CNode_474{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<None> None, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_basic_Dropout_construct_375:CNode_475{[0]: ValueNode<Primitive> Return, [1]: CNode_474}


subgraph attr:
subgraph instance: ms_iter_379 : 000001EBEE5DD540
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\_extends\parse\standard_method.py:2345/def ms_iter(xs):/
subgraph @ms_iter_379(%para121_xs) {
  %1(CNode_476) = getattr(%para121_xs, "__ms_iter__")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\_extends\parse\standard_method.py:2347/    return xs.__ms_iter__/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\_extends\parse\standard_method.py:2347/    return xs.__ms_iter__/
}
# Order:
#   1: @ms_iter_379:CNode_476{[0]: ValueNode<Primitive> getattr, [1]: param_xs, [2]: ValueNode<StringImm> __ms_iter__}
#   2: @ms_iter_379:CNode_477{[0]: ValueNode<Primitive> Return, [1]: CNode_476}


subgraph attr:
training : 1
after_block : 1
subgraph instance: check_axis_valid_388 : 000001EBEE5E24F0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_388() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
}
# Order:
#   1: @check_axis_valid_388:CNode_478{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: flatten_397 : 000001EBEE5E34E0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_397 parent: [subgraph @flatten_141]() {
  %1(CNode_479) = S_Prim_isinstance(%para99_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_480) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_481) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %4(CNode_482) = Switch(%3, @flatten_483, @flatten_484)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %5(CNode_485) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %6(CNode_486) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %7(CNode_487) = Switch(%6, @flatten_488, @flatten_489)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %8(CNode_490) = %7()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_397:CNode_479{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @flatten_397:CNode_480{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_479}
#   3: @flatten_397:CNode_481{[0]: ValueNode<Primitive> Cond, [1]: CNode_480, [2]: ValueNode<BoolImm> false}
#   4: @flatten_397:CNode_482{[0]: ValueNode<Primitive> Switch, [1]: CNode_481, [2]: ValueNode<FuncGraph> flatten_483, [3]: ValueNode<FuncGraph> flatten_484}
#   5: @flatten_397:CNode_485{[0]: CNode_482}
#   6: @flatten_397:CNode_486{[0]: ValueNode<Primitive> Cond, [1]: CNode_485, [2]: ValueNode<BoolImm> false}
#   7: @flatten_397:CNode_487{[0]: ValueNode<Primitive> Switch, [1]: CNode_486, [2]: ValueNode<FuncGraph> flatten_488, [3]: ValueNode<FuncGraph> flatten_489}
#   8: @flatten_397:CNode_490{[0]: CNode_487}
#   9: @flatten_397:CNode_491{[0]: ValueNode<Primitive> Return, [1]: CNode_490}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_400 : 000001EBEE5DDA90
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_400 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_240]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_240):S_Prim_Conv2D[out_channel: I64(32), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(1), I64(1), I64(1), I64(1)), pad_mode: I64(0), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1), pad_list: (I64(1), I64(1), I64(1), I64(1))](%para101_x, %para3_features.0.weight)
      : (<null>, <Ref[Tensor[Float32]], (32, 1, 3, 3), ref_key=:features.0.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/0-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/0-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_400:CNode_492{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_403 : 000001EBEE5DDFE0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_403 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_244]() {
  %1(CNode_493) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para102_x, %para4_features.1.gamma, %para5_features.1.beta, %para46_features.1.moving_mean, %para47_features.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.beta>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.moving_mean>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_494) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_403:CNode_493{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_features.1.gamma, [3]: param_features.1.beta, [4]: param_features.1.moving_mean, [5]: param_features.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_403:CNode_494{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_493, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_403:CNode_495{[0]: ValueNode<Primitive> Return, [1]: CNode_494}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_406 : 000001EBEE5DC000
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_406 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_244]() {
  %1(CNode_496) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_497) = Switch(%1, @mindspore_nn_layer_normalization_BatchNorm2d_construct_498, @mindspore_nn_layer_normalization_BatchNorm2d_construct_499)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_500) = %2()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_406:CNode_496{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_406:CNode_497{[0]: ValueNode<Primitive> Switch, [1]: CNode_496, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_498, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_499}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_406:CNode_500{[0]: CNode_497}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_406:CNode_501{[0]: ValueNode<Primitive> Return, [1]: CNode_500}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_410 : 000001EBEE5D6B00
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_410(%para122_, %para123_) {
  %1(CNode_503) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_502()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_410:CNode_503{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_502}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_410:CNode_504{[0]: ValueNode<Primitive> Return, [1]: CNode_503}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_415 : 000001EBEE5D8AE0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_415 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_268]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_268):S_Prim_Conv2D[out_channel: I64(64), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(1), I64(1), I64(1), I64(1)), pad_mode: I64(0), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1), pad_list: (I64(1), I64(1), I64(1), I64(1))](%para105_x, %para6_features.4.weight)
      : (<null>, <Ref[Tensor[Float32]], (64, 32, 3, 3), ref_key=:features.4.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/4-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/4-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_415:CNode_505{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_418 : 000001EBE708EF40
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_418 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_272]() {
  %1(CNode_506) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para106_x, %para7_features.5.gamma, %para8_features.5.beta, %para48_features.5.moving_mean, %para49_features.5.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.beta>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_507) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_418:CNode_506{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_features.5.gamma, [3]: param_features.5.beta, [4]: param_features.5.moving_mean, [5]: param_features.5.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_418:CNode_507{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_506, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_418:CNode_508{[0]: ValueNode<Primitive> Return, [1]: CNode_507}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_421 : 000001EBE708C4C0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_421 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_272]() {
  %1(CNode_509) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_510) = Switch(%1, @mindspore_nn_layer_normalization_BatchNorm2d_construct_511, @mindspore_nn_layer_normalization_BatchNorm2d_construct_512)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_513) = %2()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_421:CNode_509{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_421:CNode_510{[0]: ValueNode<Primitive> Switch, [1]: CNode_509, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_511, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_512}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_421:CNode_513{[0]: CNode_510}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_421:CNode_514{[0]: ValueNode<Primitive> Return, [1]: CNode_513}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_425 : 000001EBE7089F90
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_425(%para124_, %para125_) {
  %1(CNode_516) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_515()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_425:CNode_516{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_515}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_425:CNode_517{[0]: ValueNode<Primitive> Return, [1]: CNode_516}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_430 : 000001EBE7088FA0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_430 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_296]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_296):S_Prim_Conv2D[out_channel: I64(128), kernel_size: (I64(3), I64(3)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(1), I64(1), I64(1), I64(1)), pad_mode: I64(0), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1), pad_list: (I64(1), I64(1), I64(1), I64(1))](%para109_x, %para9_features.8.weight)
      : (<null>, <Ref[Tensor[Float32]], (128, 64, 3, 3), ref_key=:features.8.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/8-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/8-Conv2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_430:CNode_518{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_433 : 000001EBE7085530
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_433 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_300]() {
  %1(CNode_519) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para110_x, %para10_features.9.gamma, %para11_features.9.beta, %para50_features.9.moving_mean, %para51_features.9.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.beta>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  %2(CNode_520) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:142/                return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_433:CNode_519{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_features.9.gamma, [3]: param_features.9.beta, [4]: param_features.9.moving_mean, [5]: param_features.9.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_433:CNode_520{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_519, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_433:CNode_521{[0]: ValueNode<Primitive> Return, [1]: CNode_520}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_436 : 000001EBE7086A70
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_436 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_300]() {
  %1(CNode_522) = Cond(None, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %2(CNode_523) = Switch(%1, @mindspore_nn_layer_normalization_BatchNorm2d_construct_524, @mindspore_nn_layer_normalization_BatchNorm2d_construct_525)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  %3(CNode_526) = %2()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_436:CNode_522{[0]: ValueNode<Primitive> Cond, [1]: ValueNode<None> None, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_436:CNode_523{[0]: ValueNode<Primitive> Switch, [1]: CNode_522, [2]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_524, [3]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_525}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_436:CNode_526{[0]: CNode_523}
#   4: @mindspore_nn_layer_normalization_BatchNorm2d_construct_436:CNode_527{[0]: ValueNode<Primitive> Return, [1]: CNode_526}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_440 : 000001EBE7083AA0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_440(%para126_, %para127_) {
  %1(CNode_529) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_528()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_440:CNode_529{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_528}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_440:CNode_530{[0]: ValueNode<Primitive> Return, [1]: CNode_529}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_451 : 000001EBE707EAF0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_451 parent: [subgraph @_apply_adam_114]() {
  %1(CNode_532) = call @_apply_adam_531()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:887/                    if self.use_lazy:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:887/                    if self.use_lazy:/
}
# Order:
#   1: @_apply_adam_451:CNode_532{[0]: ValueNode<FuncGraph> _apply_adam_531}
#   2: @_apply_adam_451:CNode_533{[0]: ValueNode<Primitive> Return, [1]: CNode_532}


subgraph attr:
training : 1
subgraph instance: get_loss_459 : 000001EBEA04F520
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_459(%para128_x, %para129_weights) {
  %1(CNode_535) = call @get_loss_534()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:143/        if self.reduce and self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:143/        if self.reduce and self.average:/
}
# Order:
#   1: @get_loss_459:input_dtype{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> dtype}
#   2: @get_loss_459:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_x, [2]: ValueNode<Float> Float32}
#   3: @get_loss_459:weights{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_weights, [2]: ValueNode<Float> Float32}
#   4: @get_loss_459:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Mul, [1]: weights, [2]: x}
#   5: @get_loss_459:CNode_535{[0]: ValueNode<FuncGraph> get_loss_534}
#   6: @get_loss_459:CNode_536{[0]: ValueNode<Primitive> Return, [1]: CNode_535}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_462 : 000001EBEA046060
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_462 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_363]() {
  %1(CNode_538) = call @L_mindspore_nn_layer_basic_Dense_construct_537()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_462:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BiasAdd, [1]: x, [2]: param_L_classifier.0.bias}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_462:CNode_538{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_537}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_462:CNode_539{[0]: ValueNode<Primitive> Return, [1]: CNode_538}


subgraph attr:
subgraph instance: flatten_488 : 000001EBEE5F4920
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_488() {
  %1(CNode_540) = JoinedStr("For 'flatten', both 'start_dim' and 'end_dim' must be int.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  %2(CNode_541) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
}
# Order:
#   1: @flatten_488:CNode_540{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', both 'start_dim' and 'end_dim' must be int.}
#   2: @flatten_488:CNode_541{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_540, [3]: ValueNode<StringImm> None}
#   3: @flatten_488:CNode_542{[0]: ValueNode<Primitive> Return, [1]: CNode_541}


subgraph attr:
subgraph instance: flatten_489 : 000001EBEE5E6A00
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_489 parent: [subgraph @flatten_141]() {
  %1(CNode_544) = call @flatten_543()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_489:CNode_544{[0]: ValueNode<FuncGraph> flatten_543}
#   2: @flatten_489:CNode_545{[0]: ValueNode<Primitive> Return, [1]: CNode_544}


subgraph attr:
subgraph instance: flatten_483 : 000001EBEE5E2A40
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_483 parent: [subgraph @flatten_397]() {
  %1(CNode_479) = $(flatten_397):S_Prim_isinstance(%para99_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_480) = $(flatten_397):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_483:CNode_546{[0]: ValueNode<Primitive> Return, [1]: CNode_480}


subgraph attr:
subgraph instance: flatten_484 : 000001EBEE5E54C0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_484 parent: [subgraph @flatten_141]() {
  %1(CNode_547) = S_Prim_isinstance(%para100_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_548) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_549) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_550) = Switch(%3, @flatten_551, @flatten_552)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %5(CNode_553) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_484:CNode_547{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @flatten_484:CNode_548{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_547}
#   3: @flatten_484:CNode_549{[0]: ValueNode<Primitive> Cond, [1]: CNode_548, [2]: ValueNode<BoolImm> false}
#   4: @flatten_484:CNode_550{[0]: ValueNode<Primitive> Switch, [1]: CNode_549, [2]: ValueNode<FuncGraph> flatten_551, [3]: ValueNode<FuncGraph> flatten_552}
#   5: @flatten_484:CNode_553{[0]: CNode_550}
#   6: @flatten_484:CNode_554{[0]: ValueNode<Primitive> Return, [1]: CNode_553}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_498 : 000001EBEE5DAAC0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_498 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_244]() {
  %1(CNode_555) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para102_x, %para4_features.1.gamma, %para5_features.1.beta, %para46_features.1.moving_mean, %para47_features.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.beta>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.moving_mean>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_556) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_498:CNode_555{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_features.1.gamma, [3]: param_features.1.beta, [4]: param_features.1.moving_mean, [5]: param_features.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_498:CNode_556{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_555, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_498:CNode_557{[0]: ValueNode<Primitive> Return, [1]: CNode_556}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_499 : 000001EBEE5DA020
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_499 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_244]() {
  %1(CNode_559) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_558()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_499:CNode_559{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_558}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_499:CNode_560{[0]: ValueNode<Primitive> Return, [1]: CNode_559}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_502 : 000001EBEE5D8040
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_502 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_410]() {
  %1(CNode_562) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_561()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_502:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_phi_x}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_502:CNode_562{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_561}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_502:CNode_563{[0]: ValueNode<Primitive> Return, [1]: CNode_562}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_511 : 000001EBE708CF60
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_511 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_272]() {
  %1(CNode_564) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para106_x, %para7_features.5.gamma, %para8_features.5.beta, %para48_features.5.moving_mean, %para49_features.5.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.beta>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_565) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_511:CNode_564{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_features.5.gamma, [3]: param_features.5.beta, [4]: param_features.5.moving_mean, [5]: param_features.5.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_511:CNode_565{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_564, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_511:CNode_566{[0]: ValueNode<Primitive> Return, [1]: CNode_565}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_512 : 000001EBE708DF50
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_512 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_272]() {
  %1(CNode_568) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_567()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_512:CNode_568{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_567}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_512:CNode_569{[0]: ValueNode<Primitive> Return, [1]: CNode_568}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_515 : 000001EBE7088A50
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_515 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_425]() {
  %1(CNode_571) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_570()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_515:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_phi_x}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_515:CNode_571{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_570}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_515:CNode_572{[0]: ValueNode<Primitive> Return, [1]: CNode_571}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_524 : 000001EBE7086520
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_524 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_300]() {
  %1(CNode_573) = S_Prim_BatchNorm[is_training: Bool(1), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", side_effect_mem: Bool(1), output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para110_x, %para10_features.9.gamma, %para11_features.9.beta, %para50_features.9.moving_mean, %para51_features.9.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.beta>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  %2(CNode_574) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:155/            return self.bn_train(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_524:CNode_573{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_features.9.gamma, [3]: param_features.9.beta, [4]: param_features.9.moving_mean, [5]: param_features.9.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_524:CNode_574{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_573, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_524:CNode_575{[0]: ValueNode<Primitive> Return, [1]: CNode_574}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_525 : 000001EBE7085FD0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_525 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_300]() {
  %1(CNode_577) = call @mindspore_nn_layer_normalization_BatchNorm2d_construct_576()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:154/        if self.use_batch_statistics:/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_525:CNode_577{[0]: ValueNode<FuncGraph> mindspore_nn_layer_normalization_BatchNorm2d_construct_576}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_525:CNode_578{[0]: ValueNode<Primitive> Return, [1]: CNode_577}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_528 : 000001EBE7083FF0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_528 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_440]() {
  %1(CNode_580) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_579()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_528:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_phi_x}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_528:CNode_580{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_579}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_528:CNode_581{[0]: ValueNode<Primitive> Return, [1]: CNode_580}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_531 : 000001EBE707DB00
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_531 parent: [subgraph @_apply_adam_114]() {
  %1(CNode_583) = call @_apply_adam_582()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:894/                        if self.use_amsgrad:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:894/                        if self.use_amsgrad:/
}
# Order:
#   1: @_apply_adam_531:CNode_583{[0]: ValueNode<FuncGraph> _apply_adam_582}
#   2: @_apply_adam_531:CNode_584{[0]: ValueNode<Primitive> Return, [1]: CNode_583}


subgraph attr:
training : 1
subgraph instance: get_loss_534 : 000001EBEA051A50
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_534 parent: [subgraph @get_loss_459]() {
  %1(CNode_586) = call @get_loss_585()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
}
# Order:
#   1: @get_loss_534:CNode_587{[0]: ValueNode<FuncGraph> get_axis_588, [1]: x}
#   2: @get_loss_534:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReduceMean, [1]: x, [2]: CNode_587}
#   3: @get_loss_534:CNode_586{[0]: ValueNode<FuncGraph> get_loss_585}
#   4: @get_loss_534:CNode_589{[0]: ValueNode<Primitive> Return, [1]: CNode_586}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_537 : 000001EBEA048040
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_537 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_462]() {
  %1(CNode_591) = call @L_mindspore_nn_layer_basic_Dense_construct_590()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_537:CNode_591{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_590}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_537:CNode_592{[0]: ValueNode<Primitive> Return, [1]: CNode_591}


subgraph attr:
after_block : 1
subgraph instance: flatten_543 : 000001EBEE5E44D0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_543 parent: [subgraph @flatten_141]() {
  %1(CNode_593) = S_Prim_check_flatten_order[constexpr_prim: Bool(1)](%para98_order)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1736/    check_flatten_order_const(order)/
  %2(CNode_594) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %3(CNode_595) = S_Prim_equal(%para98_order, "F")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  %4(CNode_596) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  %5(CNode_597) = Switch(%4, @flatten_598, @flatten_599)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  %6(CNode_600) = %5()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  %7(CNode_601) = Depend[side_effect_propagate: I64(1)](%6, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @flatten_543:CNode_593{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_flatten_order, [1]: param_order}
#   2: @flatten_543:CNode_595{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_order, [2]: ValueNode<StringImm> F}
#   3: @flatten_543:CNode_596{[0]: ValueNode<Primitive> Cond, [1]: CNode_595, [2]: ValueNode<BoolImm> false}
#   4: @flatten_543:CNode_597{[0]: ValueNode<Primitive> Switch, [1]: CNode_596, [2]: ValueNode<FuncGraph> flatten_598, [3]: ValueNode<FuncGraph> flatten_599}
#   5: @flatten_543:CNode_600{[0]: CNode_597}
#   6: @flatten_543:CNode_602{[0]: ValueNode<Primitive> Return, [1]: CNode_601}


subgraph attr:
subgraph instance: flatten_551 : 000001EBEE5E5F60
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_551 parent: [subgraph @flatten_484]() {
  %1(CNode_547) = $(flatten_484):S_Prim_isinstance(%para100_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_548) = $(flatten_484):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @flatten_551:CNode_603{[0]: ValueNode<Primitive> Return, [1]: CNode_548}


subgraph attr:
subgraph instance: flatten_552 : 000001EBEE5E64B0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_552 parent: [subgraph @flatten_141]() {
  %1(CNode_604) = S_Prim_isinstance(%para99_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %2(CNode_605) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %3(CNode_606) = Switch(%2, @flatten_607, @flatten_608)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_609) = %3()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_552:CNode_604{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @flatten_552:CNode_605{[0]: ValueNode<Primitive> Cond, [1]: CNode_604, [2]: ValueNode<BoolImm> false}
#   3: @flatten_552:CNode_606{[0]: ValueNode<Primitive> Switch, [1]: CNode_605, [2]: ValueNode<FuncGraph> flatten_607, [3]: ValueNode<FuncGraph> flatten_608}
#   4: @flatten_552:CNode_609{[0]: CNode_606}
#   5: @flatten_552:CNode_610{[0]: ValueNode<Primitive> Return, [1]: CNode_609}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_558 : 000001EBEE5DCFF0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_558 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_244]() {
  %1(CNode_611) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para102_x, %para4_features.1.gamma, %para5_features.1.beta, %para46_features.1.moving_mean, %para47_features.1.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.gamma>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.beta>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.moving_mean>, <Ref[Tensor[Float32]], (32), ref_key=:features.1.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_612) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/1-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_558:CNode_611{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_features.1.gamma, [3]: param_features.1.beta, [4]: param_features.1.moving_mean, [5]: param_features.1.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_558:CNode_612{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_611, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_558:CNode_613{[0]: ValueNode<Primitive> Return, [1]: CNode_612}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_561 : 000001EBEE5D5B10
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_561 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_502]() {
  %1(CNode_614) = Cond(%para123_phi_expand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %2(CNode_615) = Switch(%1, @mindspore_nn_layer_pooling_MaxPool2d_construct_616, @mindspore_nn_layer_pooling_MaxPool2d_construct_617)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %3(CNode_618) = %2()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %4(CNode_620) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_619(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_561:CNode_614{[0]: ValueNode<Primitive> Cond, [1]: param_phi_expand_batch, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_561:CNode_615{[0]: ValueNode<Primitive> Switch, [1]: CNode_614, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_616, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_617}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_561:CNode_618{[0]: CNode_615}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_561:CNode_620{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_619, [1]: CNode_618}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_561:CNode_621{[0]: ValueNode<Primitive> Return, [1]: CNode_620}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_567 : 000001EBE708CA10
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_567 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_272]() {
  %1(CNode_622) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para106_x, %para7_features.5.gamma, %para8_features.5.beta, %para48_features.5.moving_mean, %para49_features.5.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.gamma>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.beta>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.moving_mean>, <Ref[Tensor[Float32]], (64), ref_key=:features.5.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_623) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/5-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_567:CNode_622{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_features.5.gamma, [3]: param_features.5.beta, [4]: param_features.5.moving_mean, [5]: param_features.5.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_567:CNode_623{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_622, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_567:CNode_624{[0]: ValueNode<Primitive> Return, [1]: CNode_623}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_570 : 000001EBE708B4D0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_570 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_515]() {
  %1(CNode_625) = Cond(%para125_phi_expand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %2(CNode_626) = Switch(%1, @mindspore_nn_layer_pooling_MaxPool2d_construct_627, @mindspore_nn_layer_pooling_MaxPool2d_construct_628)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %3(CNode_629) = %2()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %4(CNode_631) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_630(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_570:CNode_625{[0]: ValueNode<Primitive> Cond, [1]: param_phi_expand_batch, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_570:CNode_626{[0]: ValueNode<Primitive> Switch, [1]: CNode_625, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_627, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_628}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_570:CNode_629{[0]: CNode_626}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_570:CNode_631{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_630, [1]: CNode_629}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_570:CNode_632{[0]: ValueNode<Primitive> Return, [1]: CNode_631}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_normalization_BatchNorm2d_construct_576 : 000001EBE7088500
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:138/    def construct(self, x):/
subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_576 parent: [subgraph @mindspore_nn_layer_normalization_BatchNorm2d_construct_300]() {
  %1(CNode_633) = S_Prim_BatchNorm[is_training: Bool(0), input_names: ["x", "scale", "offset", "mean", "variance"], epsilon: F32(1e-05), momentum: F32(0.1), format: "NCHW", output_names: ["y", "batch_mean", "batch_variance", "reserve_space_1", "reserve_space_2"]](%para110_x, %para10_features.9.gamma, %para11_features.9.beta, %para50_features.9.moving_mean, %para51_features.9.moving_variance)
      : (<null>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.gamma>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.beta>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.moving_mean>, <Ref[Tensor[Float32]], (128), ref_key=:features.9.moving_variance>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  %2(CNode_634) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/9-BatchNorm2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\normalization.py:161/        return self.bn_infer(x,/
}
# Order:
#   1: @mindspore_nn_layer_normalization_BatchNorm2d_construct_576:CNode_633{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BatchNorm, [1]: param_x, [2]: param_features.9.gamma, [3]: param_features.9.beta, [4]: param_features.9.moving_mean, [5]: param_features.9.moving_variance}
#   2: @mindspore_nn_layer_normalization_BatchNorm2d_construct_576:CNode_634{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_633, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_normalization_BatchNorm2d_construct_576:CNode_635{[0]: ValueNode<Primitive> Return, [1]: CNode_634}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_579 : 000001EBE7082010
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_579 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_528]() {
  %1(CNode_636) = Cond(%para127_phi_expand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %2(CNode_637) = Switch(%1, @mindspore_nn_layer_pooling_MaxPool2d_construct_638, @mindspore_nn_layer_pooling_MaxPool2d_construct_639)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %3(CNode_640) = %2()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %4(CNode_642) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_641(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_579:CNode_636{[0]: ValueNode<Primitive> Cond, [1]: param_phi_expand_batch, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_579:CNode_637{[0]: ValueNode<Primitive> Switch, [1]: CNode_636, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_638, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_639}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_579:CNode_640{[0]: CNode_637}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_579:CNode_642{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_641, [1]: CNode_640}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_579:CNode_643{[0]: ValueNode<Primitive> Return, [1]: CNode_642}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_582 : 000001EBE707D5B0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_582 parent: [subgraph @_apply_adam_114]() {
  %1(CNode_645) = call @_apply_adam_644()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
}
# Order:
#   1: @_apply_adam_582:CNode_646{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_adam_opt, [2]: ValueNode<DoSignaturePrimitive> S_Prim_Adam, [3]: ValueNode<DoSignaturePrimitive> S_Prim_FusedSparseAdam, [4]: ValueNode<DoSignaturePrimitive> S_Prim_Push, [5]: ValueNode<DoSignaturePrimitive> S_Prim_Pull, [6]: ValueNode<BoolImm> false, [7]: ValueNode<BoolImm> false, [8]: ValueNode<BoolImm> true, [9]: param_beta1_power, [10]: param_beta2_power, [11]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.9), [12]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0.999), [13]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=1e-08), [14]: param_lr}
#   2: @_apply_adam_582:success{[0]: ValueNode<DoSignaturePrimitive> S_Prim_map, [1]: CNode_646, [2]: param_gradients, [3]: param_params, [4]: param_moment1, [5]: param_moment2, [6]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false, false, false, false, false, false), [7]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false, false, false, false, false, false)}
#   3: @_apply_adam_582:CNode_645{[0]: ValueNode<FuncGraph> _apply_adam_644}
#   4: @_apply_adam_582:CNode_647{[0]: ValueNode<Primitive> Return, [1]: CNode_645}


subgraph attr:
training : 1
subgraph instance: get_axis_588 : 000001EBEA04DFE0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:113/    def get_axis(self, x):/
subgraph @get_axis_588(%para130_x) {
  %1(shape) = call @shape_350(%para130_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:120/        shape = F.shape(x)/
  %2(length) = S_Prim_sequence_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:121/        length = F.tuple_len(shape)/
  %3(perm) = S_Prim_make_range(I64(0), %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:122/        perm = F.make_range(0, length)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:123/        return perm/
}
# Order:
#   1: @get_axis_588:shape{[0]: ValueNode<FuncGraph> shape_350, [1]: param_x}
#   2: @get_axis_588:length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_sequence_len, [1]: shape}
#   3: @get_axis_588:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: length}
#   4: @get_axis_588:CNode_648{[0]: ValueNode<Primitive> Return, [1]: perm}


subgraph attr:
training : 1
subgraph instance: get_loss_585 : 000001EBEA051FA0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_585 parent: [subgraph @get_loss_534]() {
  %1(CNode_650) = call @get_loss_649()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @get_loss_585:CNode_650{[0]: ValueNode<FuncGraph> get_loss_649}
#   2: @get_loss_585:CNode_651{[0]: ValueNode<Primitive> Return, [1]: CNode_650}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_590 : 000001EBEA0445D0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_590 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_462]() {
  %1(CNode_653) = call @L_mindspore_nn_layer_basic_Dense_construct_652()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_590:CNode_653{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_652}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_590:CNode_654{[0]: ValueNode<Primitive> Return, [1]: CNode_653}


subgraph attr:
subgraph instance: flatten_598 : 000001EBEE5F1950
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_598 parent: [subgraph @flatten_141]() {
  %1(x_rank) = S_Prim_Rank(%para97_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1738/        x_rank = rank_(input)/
  %2(CNode_655) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  %3(CNode_656) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  %4(CNode_657) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  %5(CNode_658) = Switch(%4, @flatten_659, @flatten_660)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  %6(CNode_661) = %5()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_598:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input}
#   2: @flatten_598:CNode_655{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   3: @flatten_598:CNode_656{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_655}
#   4: @flatten_598:CNode_657{[0]: ValueNode<Primitive> Cond, [1]: CNode_656, [2]: ValueNode<BoolImm> false}
#   5: @flatten_598:CNode_658{[0]: ValueNode<Primitive> Switch, [1]: CNode_657, [2]: ValueNode<FuncGraph> flatten_659, [3]: ValueNode<FuncGraph> flatten_660}
#   6: @flatten_598:CNode_661{[0]: CNode_658}
#   7: @flatten_598:CNode_662{[0]: ValueNode<Primitive> Return, [1]: CNode_661}


subgraph attr:
subgraph instance: flatten_599 : 000001EBEE5E2F90
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_599 parent: [subgraph @flatten_141]() {
  %1(CNode_664) = call @flatten_663(%para97_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @flatten_599:CNode_665{[0]: ValueNode<Primitive> Return, [1]: CNode_664}
#   2: @flatten_599:CNode_664{[0]: ValueNode<FuncGraph> flatten_663, [1]: param_input}


subgraph attr:
subgraph instance: flatten_607 : 000001EBEE5E3F80
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_607 parent: [subgraph @flatten_552]() {
  %1(CNode_604) = $(flatten_552):S_Prim_isinstance(%para99_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @flatten_607:CNode_666{[0]: ValueNode<Primitive> Return, [1]: CNode_604}


subgraph attr:
subgraph instance: flatten_608 : 000001EBEE5E5A10
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_608 parent: [subgraph @flatten_141]() {
  %1(CNode_667) = S_Prim_isinstance(%para100_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_608:CNode_667{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @flatten_608:CNode_668{[0]: ValueNode<Primitive> Return, [1]: CNode_667}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_619 : 000001EBEE5D9AD0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_619(%para131_) {
  %1(CNode_670) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_669()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_619:CNode_670{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_669}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_619:CNode_671{[0]: ValueNode<Primitive> Return, [1]: CNode_670}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_616 : 000001EBEE5D55C0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_616 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_502]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_502):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para122_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_672) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_673) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_674) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_675, @mindspore_nn_layer_pooling_MaxPool2d_construct_676)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_677) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_679) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_678(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_616:CNode_672{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_616:CNode_673{[0]: ValueNode<Primitive> Cond, [1]: CNode_672, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_616:CNode_674{[0]: ValueNode<Primitive> Switch, [1]: CNode_673, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_675, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_676}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_616:CNode_677{[0]: CNode_674}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_616:CNode_679{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_678, [1]: CNode_677}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_616:CNode_680{[0]: ValueNode<Primitive> Return, [1]: CNode_679}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_617 : 000001EBEE5D65B0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_617 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_502]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_502):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para122_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_617:CNode_681{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_630 : 000001EBE708F490
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_630(%para132_) {
  %1(CNode_683) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_682()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_630:CNode_683{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_682}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_630:CNode_684{[0]: ValueNode<Primitive> Return, [1]: CNode_683}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_627 : 000001EBE708AA30
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_627 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_515]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_515):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para124_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_685) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_686) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_687) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_688, @mindspore_nn_layer_pooling_MaxPool2d_construct_689)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_690) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_692) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_691(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_627:CNode_685{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_627:CNode_686{[0]: ValueNode<Primitive> Cond, [1]: CNode_685, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_627:CNode_687{[0]: ValueNode<Primitive> Switch, [1]: CNode_686, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_688, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_689}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_627:CNode_690{[0]: CNode_687}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_627:CNode_692{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_691, [1]: CNode_690}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_627:CNode_693{[0]: ValueNode<Primitive> Return, [1]: CNode_692}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_628 : 000001EBE70894F0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_628 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_515]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_515):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para124_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_628:CNode_694{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_641 : 000001EBE7084A90
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_641(%para133_) {
  %1(CNode_696) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_695()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_641:CNode_696{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_695}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_641:CNode_697{[0]: ValueNode<Primitive> Return, [1]: CNode_696}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_638 : 000001EBE7084540
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_638 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_528]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_528):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para126_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_698) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_699) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_700) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_701, @mindspore_nn_layer_pooling_MaxPool2d_construct_702)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_703) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_705) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_704(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_638:CNode_698{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_638:CNode_699{[0]: ValueNode<Primitive> Cond, [1]: CNode_698, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_638:CNode_700{[0]: ValueNode<Primitive> Switch, [1]: CNode_699, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_701, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_702}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_638:CNode_703{[0]: CNode_700}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_638:CNode_705{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_704, [1]: CNode_703}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_638:CNode_706{[0]: ValueNode<Primitive> Return, [1]: CNode_705}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_639 : 000001EBE7080580
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_639 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_528]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_528):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para126_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_639:CNode_707{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_644 : 000001EBE707B080
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_644 parent: [subgraph @_apply_adam_582]() {
  %1(CNode_709) = call @_apply_adam_708()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:894/                        if self.use_amsgrad:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:894/                        if self.use_amsgrad:/
}
# Order:
#   1: @_apply_adam_644:CNode_709{[0]: ValueNode<FuncGraph> _apply_adam_708}
#   2: @_apply_adam_644:CNode_710{[0]: ValueNode<Primitive> Return, [1]: CNode_709}


subgraph attr:
training : 1
subgraph instance: get_loss_649 : 000001EBEA04E530
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_649 parent: [subgraph @get_loss_534]() {
  %1(CNode_712) = call @get_loss_711()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @get_loss_649:CNode_712{[0]: ValueNode<FuncGraph> get_loss_711}
#   2: @get_loss_649:CNode_713{[0]: ValueNode<Primitive> Return, [1]: CNode_712}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_652 : 000001EBEA048590
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_652 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_462]() {
  %1(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_186):S_Prim_Shape(%para116_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_714) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %3(CNode_715) = S_Prim_not_equal(%2, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %4(CNode_716) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %5(CNode_717) = Switch(%4, @L_mindspore_nn_layer_basic_Dense_construct_718, @L_mindspore_nn_layer_basic_Dense_construct_719)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %6(CNode_720) = %5()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %7(CNode_722) = call @L_mindspore_nn_layer_basic_Dense_construct_721(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\container.py:294/        for cell in self.cell_list:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_652:CNode_714{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_652:CNode_715{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_714, [2]: ValueNode<Int64Imm> 2}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_652:CNode_716{[0]: ValueNode<Primitive> Cond, [1]: CNode_715, [2]: ValueNode<BoolImm> false}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_652:CNode_717{[0]: ValueNode<Primitive> Switch, [1]: CNode_716, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_718, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_719}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_652:CNode_720{[0]: CNode_717}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_652:CNode_722{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_721, [1]: CNode_720}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_652:CNode_723{[0]: ValueNode<Primitive> Return, [1]: CNode_722}


subgraph attr:
subgraph instance: flatten_659 : 000001EBEE5EFEC0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_659 parent: [subgraph @flatten_141]() {
  %1(CNode_724) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1741/            return reshape_(input, (-1,))/
  %2(CNode_725) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1741/            return reshape_(input, (-1,))/
  %3(CNode_726) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para97_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1741/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1741/            return reshape_(input, (-1,))/
}
# Order:
#   1: @flatten_659:CNode_724{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_659:CNode_725{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_724}
#   3: @flatten_659:CNode_726{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_input, [2]: CNode_725}
#   4: @flatten_659:CNode_727{[0]: ValueNode<Primitive> Return, [1]: CNode_726}


subgraph attr:
subgraph instance: flatten_660 : 000001EBEE5F2940
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_660 parent: [subgraph @flatten_598]() {
  %1(CNode_729) = call @flatten_728()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_660:CNode_729{[0]: ValueNode<FuncGraph> flatten_728}
#   2: @flatten_660:CNode_730{[0]: ValueNode<Primitive> Return, [1]: CNode_729}


subgraph attr:
after_block : 1
subgraph instance: flatten_663 : 000001EBEE5E74A0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_663 parent: [subgraph @flatten_141](%para134_) {
  %1(CNode_731) = S_Prim_equal(%para99_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_732) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %3(CNode_733) = Switch(%2, @flatten_734, @flatten_735)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %4(CNode_736) = %3()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %5(CNode_737) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %6(CNode_738) = Switch(%5, @flatten_739, @flatten_740)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %7(CNode_741) = %6()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_663:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_phi_input}
#   2: @flatten_663:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_phi_input}
#   3: @flatten_663:CNode_731{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_start_dim, [2]: ValueNode<Int64Imm> 1}
#   4: @flatten_663:CNode_732{[0]: ValueNode<Primitive> Cond, [1]: CNode_731, [2]: ValueNode<BoolImm> false}
#   5: @flatten_663:CNode_733{[0]: ValueNode<Primitive> Switch, [1]: CNode_732, [2]: ValueNode<FuncGraph> flatten_734, [3]: ValueNode<FuncGraph> flatten_735}
#   6: @flatten_663:CNode_736{[0]: CNode_733}
#   7: @flatten_663:CNode_737{[0]: ValueNode<Primitive> Cond, [1]: CNode_736, [2]: ValueNode<BoolImm> false}
#   8: @flatten_663:CNode_738{[0]: ValueNode<Primitive> Switch, [1]: CNode_737, [2]: ValueNode<FuncGraph> flatten_739, [3]: ValueNode<FuncGraph> flatten_740}
#   9: @flatten_663:CNode_741{[0]: CNode_738}
#  10: @flatten_663:CNode_742{[0]: ValueNode<Primitive> Return, [1]: CNode_741}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_669 : 000001EBEE5DC550
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_669 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_619]() {
  %1(CNode_744) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_743()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_669:CNode_744{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_743}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_669:CNode_745{[0]: ValueNode<Primitive> Return, [1]: CNode_744}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_678 : 000001EBEE5D6060
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_678(%para135_) {
  Return(%para135_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_678:CNode_746{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_675 : 000001EBEE5D9030
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_675 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_502]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_502):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para122_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_747) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_748) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_749) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_750) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_751) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_752) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_675:CNode_747{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_675:CNode_748{[0]: ValueNode<Primitive> getattr, [1]: CNode_747, [2]: ValueNode<StringImm> squeeze}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_675:CNode_749{[0]: CNode_748, [1]: ValueNode<Int64Imm> 0}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_675:CNode_750{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_675:CNode_751{[0]: ValueNode<Primitive> getattr, [1]: CNode_750, [2]: ValueNode<StringImm> squeeze}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_675:CNode_752{[0]: CNode_751, [1]: ValueNode<Int64Imm> 0}
#   7: @mindspore_nn_layer_pooling_MaxPool2d_construct_675:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_749, [2]: CNode_752}
#   8: @mindspore_nn_layer_pooling_MaxPool2d_construct_675:CNode_753{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_676 : 000001EBEE5D5070
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_676 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_502]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_502):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para122_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_754) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_676:CNode_754{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_676:out{[0]: CNode_754, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_676:CNode_755{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_682 : 000001EBE708DA00
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_682 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_630]() {
  %1(CNode_757) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_756()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_682:CNode_757{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_756}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_682:CNode_758{[0]: ValueNode<Primitive> Return, [1]: CNode_757}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_691 : 000001EBE7087FB0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_691(%para136_) {
  Return(%para136_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_691:CNode_759{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_688 : 000001EBE708BA20
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_688 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_515]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_515):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para124_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_760) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_761) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_762) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_763) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_764) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_765) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_688:CNode_760{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_688:CNode_761{[0]: ValueNode<Primitive> getattr, [1]: CNode_760, [2]: ValueNode<StringImm> squeeze}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_688:CNode_762{[0]: CNode_761, [1]: ValueNode<Int64Imm> 0}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_688:CNode_763{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_688:CNode_764{[0]: ValueNode<Primitive> getattr, [1]: CNode_763, [2]: ValueNode<StringImm> squeeze}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_688:CNode_765{[0]: CNode_764, [1]: ValueNode<Int64Imm> 0}
#   7: @mindspore_nn_layer_pooling_MaxPool2d_construct_688:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_762, [2]: CNode_765}
#   8: @mindspore_nn_layer_pooling_MaxPool2d_construct_688:CNode_766{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_689 : 000001EBE708AF80
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_689 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_515]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_515):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para124_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_767) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_689:CNode_767{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_689:out{[0]: CNode_767, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_689:CNode_768{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_695 : 000001EBE7082560
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_695 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_641]() {
  %1(CNode_770) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_769()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_695:CNode_770{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_769}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_695:CNode_771{[0]: ValueNode<Primitive> Return, [1]: CNode_770}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_704 : 000001EBE7083000
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_704(%para137_) {
  Return(%para137_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_704:CNode_772{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_701 : 000001EBE7082AB0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_701 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_528]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_528):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para126_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_773) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_774) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_775) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_776) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_777) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_778) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_701:CNode_773{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_701:CNode_774{[0]: ValueNode<Primitive> getattr, [1]: CNode_773, [2]: ValueNode<StringImm> squeeze}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_701:CNode_775{[0]: CNode_774, [1]: ValueNode<Int64Imm> 0}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_701:CNode_776{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_701:CNode_777{[0]: ValueNode<Primitive> getattr, [1]: CNode_776, [2]: ValueNode<StringImm> squeeze}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_701:CNode_778{[0]: CNode_777, [1]: ValueNode<Int64Imm> 0}
#   7: @mindspore_nn_layer_pooling_MaxPool2d_construct_701:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_775, [2]: CNode_778}
#   8: @mindspore_nn_layer_pooling_MaxPool2d_construct_701:CNode_779{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_702 : 000001EBE7080AD0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_702 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_528]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_528):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para126_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_780) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_702:CNode_780{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_702:out{[0]: CNode_780, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_702:CNode_781{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_708 : 000001EBE707F040
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_708 parent: [subgraph @_apply_adam_582]() {
  %1(CNode_783) = call @_apply_adam_782()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:887/                    if self.use_lazy:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:887/                    if self.use_lazy:/
}
# Order:
#   1: @_apply_adam_708:CNode_783{[0]: ValueNode<FuncGraph> _apply_adam_782}
#   2: @_apply_adam_708:CNode_784{[0]: ValueNode<Primitive> Return, [1]: CNode_783}


subgraph attr:
training : 1
subgraph instance: get_loss_711 : 000001EBEA0524F0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_711 parent: [subgraph @get_loss_534]() {
  %1(weights) = $(get_loss_459):S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%para129_weights, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:141/        weights = self.cast(weights, mstype.float32)/
  %2(x) = $(get_loss_459):S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%para128_x, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:140/        x = self.cast(x, mstype.float32)/
  %3(x) = $(get_loss_459):S_Prim_Mul[input_names: ["x", "y"], output_names: ["output"]](%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:142/        x = self.mul(weights, x)/
  %4(CNode_587) = $(get_loss_534):call @get_axis_588(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %5(x) = $(get_loss_534):S_Prim_ReduceMean[keep_dims: Bool(0), input_names: ["input_x", "axis"], output_names: ["y"]](%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %6(input_dtype) = $(get_loss_459):getattr(%para128_x, "dtype")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:139/        input_dtype = x.dtype/
  %7(x) = S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:147/        x = self.cast(x, input_dtype)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\loss\loss.py:148/        return x/
}
# Order:
#   1: @get_loss_711:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: x, [2]: input_dtype}
#   2: @get_loss_711:CNode_785{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_721 : 000001EBEA049580
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_721(%para138_) {
  Return(%para138_phi_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:635/        return x/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_721:CNode_786{[0]: ValueNode<Primitive> Return, [1]: param_phi_x}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_718 : 000001EBEA049030
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_718 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_462]() {
  %1(x) = $(L_mindspore_nn_layer_basic_Dense_construct_363):S_Prim_MatMul[transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_b: Bool(1), output_names: ["output"], transpose_x1: Bool(0), transpose_x2: Bool(1)](%para120_phi_x, %para118_L_classifier.0.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_mindspore_nn_layer_basic_Dense_construct_462):S_Prim_BiasAdd[format: "NCHW", input_names: ["x", "b"], output_names: ["output"], data_format: "NCHW"](%1, %para117_L_classifier.0.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  %3(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_186):S_Prim_Shape(%para116_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %4(CNode_787) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %5(CNode_788) = S_Prim_make_slice(None, %4, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %6(CNode_789) = S_Prim_getitem(%3, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %7(CNode_791) = call @L_shape_790(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %8(CNode_792) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %9(CNode_793) = S_Prim_getitem(%7, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %10(CNode_794) = S_Prim_MakeTuple(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %11(out_shape) = S_Prim_add(%6, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %12(x) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%2, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:634/            x = self.reshape(x, out_shape)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_718:CNode_787{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_718:CNode_788{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: CNode_787, [3]: ValueNode<None> None}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_718:CNode_789{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_788}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_718:CNode_791{[0]: ValueNode<FuncGraph> L_shape_790, [1]: x}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_718:CNode_792{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_718:CNode_793{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_791, [2]: CNode_792}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_718:CNode_794{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_793}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_718:out_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_789, [2]: CNode_794}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_718:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: x, [2]: out_shape}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_718:CNode_795{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_719 : 000001EBEA048AE0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_719 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_462]() {
  %1(x) = $(L_mindspore_nn_layer_basic_Dense_construct_363):S_Prim_MatMul[transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_b: Bool(1), output_names: ["output"], transpose_x1: Bool(0), transpose_x2: Bool(1)](%para120_phi_x, %para118_L_classifier.0.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_mindspore_nn_layer_basic_Dense_construct_462):S_Prim_BiasAdd[format: "NCHW", input_names: ["x", "b"], output_names: ["output"], data_format: "NCHW"](%1, %para117_L_classifier.0.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/classifier-SequentialCell/0-Dense)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_719:CNode_796{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
after_block : 1
subgraph instance: flatten_728 : 000001EBEE5F3930
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_728 parent: [subgraph @flatten_598]() {
  %1(CNode_798) = call @_get_cache_prim_797(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %2(CNode_799) = %1()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %3(x_rank) = $(flatten_598):S_Prim_Rank(%para97_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1738/        x_rank = rank_(input)/
  %4(perm) = S_Prim_make_range(I64(0), %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1742/        perm = ops.make_range(0, x_rank)/
  %5(new_order) = S_Prim_tuple_reversed(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1743/        new_order = ops.tuple_reversed(perm)/
  %6(input) = %2(%para97_input, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %7(CNode_800) = call @flatten_663(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1738/        x_rank = rank_(input)/
}
# Order:
#   1: @flatten_728:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: x_rank}
#   2: @flatten_728:new_order{[0]: ValueNode<DoSignaturePrimitive> S_Prim_tuple_reversed, [1]: perm}
#   3: @flatten_728:CNode_798{[0]: ValueNode<FuncGraph> _get_cache_prim_797, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.array_ops.Transpose'}
#   4: @flatten_728:CNode_799{[0]: CNode_798}
#   5: @flatten_728:input{[0]: CNode_799, [1]: param_input, [2]: new_order}
#   6: @flatten_728:CNode_801{[0]: ValueNode<Primitive> Return, [1]: CNode_800}
#   7: @flatten_728:CNode_800{[0]: ValueNode<FuncGraph> flatten_663, [1]: input}


subgraph attr:
subgraph instance: flatten_739 : 000001EBEE5F1EA0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_739 parent: [subgraph @flatten_663]() {
  %1(x_rank) = $(flatten_663):S_Prim_Rank(%para134_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(CNode_802) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  %3(CNode_803) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  %4(CNode_804) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  %5(CNode_805) = Switch(%4, @flatten_806, @flatten_807)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  %6(CNode_808) = %5()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_739:CNode_802{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   2: @flatten_739:CNode_803{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_802}
#   3: @flatten_739:CNode_804{[0]: ValueNode<Primitive> Cond, [1]: CNode_803, [2]: ValueNode<BoolImm> false}
#   4: @flatten_739:CNode_805{[0]: ValueNode<Primitive> Switch, [1]: CNode_804, [2]: ValueNode<FuncGraph> flatten_806, [3]: ValueNode<FuncGraph> flatten_807}
#   5: @flatten_739:CNode_808{[0]: CNode_805}
#   6: @flatten_739:CNode_809{[0]: ValueNode<Primitive> Return, [1]: CNode_808}


subgraph attr:
subgraph instance: flatten_740 : 000001EBEE5E7F40
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_740 parent: [subgraph @flatten_663]() {
  %1(CNode_811) = call @flatten_810()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_740:CNode_811{[0]: ValueNode<FuncGraph> flatten_810}
#   2: @flatten_740:CNode_812{[0]: ValueNode<Primitive> Return, [1]: CNode_811}


subgraph attr:
subgraph instance: flatten_734 : 000001EBEE5E79F0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_734 parent: [subgraph @flatten_141]() {
  %1(CNode_813) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_814) = S_Prim_equal(%para100_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_734:CNode_813{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_734:CNode_814{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_end_dim, [2]: CNode_813}
#   3: @flatten_734:CNode_815{[0]: ValueNode<Primitive> Return, [1]: CNode_814}


subgraph attr:
subgraph instance: flatten_735 : 000001EBEE5E6F50
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_735 parent: [subgraph @flatten_663]() {
  %1(CNode_731) = $(flatten_663):S_Prim_equal(%para99_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_735:CNode_816{[0]: ValueNode<Primitive> Return, [1]: CNode_731}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_743 : 000001EBEE5DA570
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_743 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_619]() {
  Return(%para131_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/3-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:589/        return out/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_743:CNode_817{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_756 : 000001EBE708F9E0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_756 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_630]() {
  Return(%para132_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/7-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:589/        return out/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_756:CNode_818{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_769 : 000001EBE7081AC0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_769 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_641]() {
  Return(%para133_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/features-SequentialCell/11-MaxPool2d)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\pooling.py:589/        return out/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_769:CNode_819{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_782 : 000001EBE707B5D0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_782 parent: [subgraph @_apply_adam_582]() {
  %1(CNode_821) = call @_apply_adam_820()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:866/                if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:866/                if self.is_group_lr:/
}
# Order:
#   1: @_apply_adam_782:CNode_821{[0]: ValueNode<FuncGraph> _apply_adam_820}
#   2: @_apply_adam_782:CNode_822{[0]: ValueNode<Primitive> Return, [1]: CNode_821}


subgraph attr:
subgraph instance: L_shape_790 : 000001EBEA0455C0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1484/def shape(input_x):/
subgraph @L_shape_790(%para139_input_x) {
  %1(CNode_456) = S_Prim_Shape(%para139_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1510/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1510/    return shape_(input_x)/
}
# Order:
#   1: @L_shape_790:CNode_456{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @L_shape_790:CNode_457{[0]: ValueNode<Primitive> Return, [1]: CNode_456}


subgraph attr:
subgraph instance: _get_cache_prim_797 : 000001EBEE5EF970
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @_get_cache_prim_797(%para140_cls) {
  %1(CNode_824) = call @_get_cache_prim_823()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
}
# Order:
#   1: @_get_cache_prim_797:CNode_824{[0]: ValueNode<FuncGraph> _get_cache_prim_823}
#   2: @_get_cache_prim_797:CNode_825{[0]: ValueNode<Primitive> Return, [1]: CNode_824}


subgraph attr:
subgraph instance: flatten_806 : 000001EBEE5F1400
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_806 parent: [subgraph @flatten_663]() {
  %1(CNode_826) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1751/            return reshape_(input, (-1,))/
  %2(CNode_827) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1751/            return reshape_(input, (-1,))/
  %3(CNode_828) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para134_phi_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1751/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1751/            return reshape_(input, (-1,))/
}
# Order:
#   1: @flatten_806:CNode_826{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_806:CNode_827{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_826}
#   3: @flatten_806:CNode_828{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_phi_input, [2]: CNode_827}
#   4: @flatten_806:CNode_829{[0]: ValueNode<Primitive> Return, [1]: CNode_828}


subgraph attr:
subgraph instance: flatten_807 : 000001EBEE5F23F0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_807 parent: [subgraph @flatten_663]() {
  %1(CNode_831) = call @flatten_830()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_807:CNode_831{[0]: ValueNode<FuncGraph> flatten_830}
#   2: @flatten_807:CNode_832{[0]: ValueNode<Primitive> Return, [1]: CNode_831}


subgraph attr:
after_block : 1
subgraph instance: flatten_810 : 000001EBEE5E8490
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_810 parent: [subgraph @flatten_663]() {
  %1(x_rank) = $(flatten_663):S_Prim_Rank(%para134_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = call @canonicalize_axis_833(%para99_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = call @canonicalize_axis_833(%para100_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_835) = call @check_dim_valid_834(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1757/    check_dim_valid(start_dim, end_dim)/
  %5(CNode_836) = StopGradient(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %6(CNode_837) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %7(CNode_838) = S_Prim_in(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %8(CNode_839) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %9(CNode_840) = Switch(%8, @flatten_841, @flatten_842)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %10(CNode_843) = %9()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %11(CNode_844) = Depend[side_effect_propagate: I64(1)](%10, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_810:idx{[0]: ValueNode<FuncGraph> canonicalize_axis_833, [1]: param_start_dim, [2]: x_rank}
#   2: @flatten_810:end_dim{[0]: ValueNode<FuncGraph> canonicalize_axis_833, [1]: param_end_dim, [2]: x_rank}
#   3: @flatten_810:CNode_835{[0]: ValueNode<FuncGraph> check_dim_valid_834, [1]: idx, [2]: end_dim}
#   4: @flatten_810:CNode_837{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   5: @flatten_810:CNode_838{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_837}
#   6: @flatten_810:CNode_839{[0]: ValueNode<Primitive> Cond, [1]: CNode_838, [2]: ValueNode<BoolImm> false}
#   7: @flatten_810:CNode_840{[0]: ValueNode<Primitive> Switch, [1]: CNode_839, [2]: ValueNode<FuncGraph> flatten_841, [3]: ValueNode<FuncGraph> flatten_842}
#   8: @flatten_810:CNode_843{[0]: CNode_840}
#   9: @flatten_810:CNode_845{[0]: ValueNode<Primitive> Return, [1]: CNode_844}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_820 : 000001EBE707F590
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_820 parent: [subgraph @_apply_adam_582]() {
  %1(CNode_847) = call @_apply_adam_846()
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:826/            if self.use_dist_optimizer:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:826/            if self.use_dist_optimizer:/
}
# Order:
#   1: @_apply_adam_820:CNode_847{[0]: ValueNode<FuncGraph> _apply_adam_846}
#   2: @_apply_adam_820:CNode_848{[0]: ValueNode<Primitive> Return, [1]: CNode_847}


subgraph attr:
subgraph instance: _get_cache_prim_823 : 000001EBEE5F33E0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @_get_cache_prim_823 parent: [subgraph @_get_cache_prim_797]() {
  Return(@_new_prim_for_graph_849)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\_primitive_cache.py:89/        return _new_prim_for_graph/
}
# Order:
#   1: @_get_cache_prim_823:CNode_850{[0]: ValueNode<Primitive> Return, [1]: ValueNode<FuncGraph> _new_prim_for_graph_849}


subgraph attr:
after_block : 1
subgraph instance: flatten_830 : 000001EBEE5F2E90
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_830 parent: [subgraph @flatten_663]() {
  %1(CNode_851) = call @_get_cache_prim_797(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %2(CNode_852) = %1()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %3(CNode_853) = %2(%para134_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
}
# Order:
#   1: @flatten_830:CNode_851{[0]: ValueNode<FuncGraph> _get_cache_prim_797, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.nn_ops.Flatten'}
#   2: @flatten_830:CNode_852{[0]: CNode_851}
#   3: @flatten_830:CNode_853{[0]: CNode_852, [1]: param_phi_input}
#   4: @flatten_830:CNode_854{[0]: ValueNode<Primitive> Return, [1]: CNode_853}


subgraph attr:
subgraph instance: check_dim_valid_834 : 000001EBEE5EF420
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_834(%para141_start_dim, %para142_end_dim) {
  %1(CNode_855) = S_Prim_greater(%para141_start_dim, %para142_end_dim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  %2(CNode_856) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  %3(CNode_857) = Switch(%2, @check_dim_valid_858, @check_dim_valid_859)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  %4(CNode_860) = %3()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @check_dim_valid_834:CNode_855{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater, [1]: param_start_dim, [2]: param_end_dim}
#   2: @check_dim_valid_834:CNode_856{[0]: ValueNode<Primitive> Cond, [1]: CNode_855, [2]: ValueNode<BoolImm> false}
#   3: @check_dim_valid_834:CNode_857{[0]: ValueNode<Primitive> Switch, [1]: CNode_856, [2]: ValueNode<FuncGraph> check_dim_valid_858, [3]: ValueNode<FuncGraph> check_dim_valid_859}
#   4: @check_dim_valid_834:CNode_860{[0]: CNode_857}
#   5: @check_dim_valid_834:CNode_861{[0]: ValueNode<Primitive> Return, [1]: CNode_860}


subgraph attr:
subgraph instance: canonicalize_axis_833 : 000001EBEE5EE430
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
subgraph @canonicalize_axis_833(%para143_axis, %para144_x_rank) {
  %1(CNode_862) = S_Prim_not_equal(%para144_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_863) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_864) = Switch(%2, @canonicalize_axis_865, @canonicalize_axis_866)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = %3()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_868) = call @check_axis_valid_867(%para143_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1727/        check_axis_valid(axis, ndim)/
  %6(CNode_869) = StopGradient(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
  %7(CNode_870) = S_Prim_greater_equal(%para143_axis, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %8(CNode_871) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %9(CNode_872) = Switch(%8, @canonicalize_axis_873, @canonicalize_axis_874)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %10(CNode_875) = %9()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %11(CNode_876) = Depend[side_effect_propagate: I64(1)](%10, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_833:CNode_862{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: param_x_rank, [2]: ValueNode<Int64Imm> 0}
#   2: @canonicalize_axis_833:CNode_863{[0]: ValueNode<Primitive> Cond, [1]: CNode_862, [2]: ValueNode<BoolImm> false}
#   3: @canonicalize_axis_833:CNode_864{[0]: ValueNode<Primitive> Switch, [1]: CNode_863, [2]: ValueNode<FuncGraph> canonicalize_axis_865, [3]: ValueNode<FuncGraph> canonicalize_axis_866}
#   4: @canonicalize_axis_833:ndim{[0]: CNode_864}
#   5: @canonicalize_axis_833:CNode_868{[0]: ValueNode<FuncGraph> check_axis_valid_867, [1]: param_axis, [2]: ndim}
#   6: @canonicalize_axis_833:CNode_870{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: ValueNode<Int64Imm> 0}
#   7: @canonicalize_axis_833:CNode_871{[0]: ValueNode<Primitive> Cond, [1]: CNode_870, [2]: ValueNode<BoolImm> false}
#   8: @canonicalize_axis_833:CNode_872{[0]: ValueNode<Primitive> Switch, [1]: CNode_871, [2]: ValueNode<FuncGraph> canonicalize_axis_873, [3]: ValueNode<FuncGraph> canonicalize_axis_874}
#   9: @canonicalize_axis_833:CNode_875{[0]: CNode_872}
#  10: @canonicalize_axis_833:CNode_877{[0]: ValueNode<Primitive> Return, [1]: CNode_876}


subgraph attr:
subgraph instance: flatten_841 : 000001EBEE5EA470
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_841 parent: [subgraph @flatten_663]() {
  %1(CNode_878) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1760/        return reshape_(input, (-1,))/
  %2(CNode_879) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1760/        return reshape_(input, (-1,))/
  %3(CNode_880) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para134_phi_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1760/        return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1760/        return reshape_(input, (-1,))/
}
# Order:
#   1: @flatten_841:CNode_878{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_841:CNode_879{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_878}
#   3: @flatten_841:CNode_880{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_phi_input, [2]: CNode_879}
#   4: @flatten_841:CNode_881{[0]: ValueNode<Primitive> Return, [1]: CNode_880}


subgraph attr:
subgraph instance: flatten_842 : 000001EBEE5E4A20
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_842 parent: [subgraph @flatten_810]() {
  %1(CNode_883) = call @flatten_882()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_842:CNode_883{[0]: ValueNode<FuncGraph> flatten_882}
#   2: @flatten_842:CNode_884{[0]: ValueNode<Primitive> Return, [1]: CNode_883}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: _apply_adam_846 : 000001EBE707BB20
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:815/    def _apply_adam(self, params, beta1_power, beta2_power, moment1, moment2, lr, gradients):/
subgraph @_apply_adam_846 parent: [subgraph @_apply_adam_582]() {
  %1(CNode_646) = $(_apply_adam_582):S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_adam_opt, S_Prim_Adam[use_locking: Bool(0), use_nesterov: Bool(0), side_effect_mem: Bool(1)], S_Prim_FusedSparseAdam[use_locking: Bool(0), use_nesterov: Bool(0), input_names: ["var", "m", "v", "beta1_power", "beta2_power", "lr", "beta1", "beta2", "epsilon", "grad", "indices"], output_names: ["var", "m", "v"], side_effect_mem: Bool(1), primitive_target: "CPU"], S_Prim_Push[optim_type: "Adam", only_shape_indices: [I64(0), I64(1), I64(2)], output_names: ["key"], side_effect_hidden: Bool(1), primitive_target: "CPU", input_names: ["optim_inputs", "optim_input_shapes"], use_nesterov: Bool(0)], S_Prim_Pull[primitive_target: "CPU", input_names: ["key", "weight"], output_names: ["output"]], Bool(0), Bool(0), Bool(1), %para80_beta1_power, %para81_beta2_power, Tensor(shape=[], dtype=Float32, value=0.9), Tensor(shape=[], dtype=Float32, value=0.999), Tensor(shape=[], dtype=Float32, value=1e-08), %para84_lr)
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
  %2(success) = $(_apply_adam_582):S_Prim_map(%1, %para85_gradients, %para79_params, %para82_moment1, %para83_moment2, (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)), (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)))
      : (<null>, <null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:901/                            success = self.map_(F.partial(_adam_opt, self.opt, self.sparse_opt, self._ps_push,/
  Return(%2)
      : (<null>)
      #scope: (Default/optimizer-Adam)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\optim\adam.py:907/        return success/
}
# Order:
#   1: @_apply_adam_846:CNode_885{[0]: ValueNode<Primitive> Return, [1]: success}


subgraph attr:
subgraph instance: _new_prim_for_graph_849 : 000001EBEE5F0EB0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\_primitive_cache.py:67/    def _new_prim_for_graph(*args, **kwargs) -> Primitive:/
subgraph @_new_prim_for_graph_849 parent: [subgraph @_get_cache_prim_797](%para145_args, %para146_kwargs) {
  %1(CNode_886) = UnpackCall_unpack_call(%para140_cls, %para145_args, %para146_kwargs)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\_primitive_cache.py:68/        return cls(*args, **kwargs)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\_primitive_cache.py:68/        return cls(*args, **kwargs)/
}
# Order:
#   1: @_new_prim_for_graph_849:CNode_886{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.887, [1]: param_cls, [2]: param_args, [3]: param_kwargs}
#   2: @_new_prim_for_graph_849:CNode_888{[0]: ValueNode<Primitive> Return, [1]: CNode_886}


subgraph attr:
subgraph instance: check_dim_valid_858 : 000001EBEE5F0410
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_858() {
  %1(CNode_889) = raise[side_effect_io: Bool(1)]("ValueError", "For 'flatten', 'start_dim' cannot come after 'end_dim'.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
}
# Order:
#   1: @check_dim_valid_858:CNode_889{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> For 'flatten', 'start_dim' cannot come after 'end_dim'., [3]: ValueNode<StringImm> None}
#   2: @check_dim_valid_858:CNode_890{[0]: ValueNode<Primitive> Return, [1]: CNode_889}


subgraph attr:
subgraph instance: check_dim_valid_859 : 000001EBEE5EB9B0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_859() {
  %1(CNode_892) = call @check_dim_valid_891()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @check_dim_valid_859:CNode_892{[0]: ValueNode<FuncGraph> check_dim_valid_891}
#   2: @check_dim_valid_859:CNode_893{[0]: ValueNode<Primitive> Return, [1]: CNode_892}


subgraph attr:
subgraph instance: check_axis_valid_867 : 000001EBEE5EBF00
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_867(%para147_axis, %para148_ndim) {
  %1(CNode_894) = S_Prim_negative(%para148_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_895) = S_Prim_less(%para147_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %3(CNode_896) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %4(CNode_897) = Switch(%3, @check_axis_valid_898, @check_axis_valid_899)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %5(CNode_900) = %4()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %6(CNode_901) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %7(CNode_902) = Switch(%6, @check_axis_valid_903, @check_axis_valid_904)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %8(CNode_905) = %7()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_867:CNode_894{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_867:CNode_895{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_894}
#   3: @check_axis_valid_867:CNode_896{[0]: ValueNode<Primitive> Cond, [1]: CNode_895, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_867:CNode_897{[0]: ValueNode<Primitive> Switch, [1]: CNode_896, [2]: ValueNode<FuncGraph> check_axis_valid_898, [3]: ValueNode<FuncGraph> check_axis_valid_899}
#   5: @check_axis_valid_867:CNode_900{[0]: CNode_897}
#   6: @check_axis_valid_867:CNode_901{[0]: ValueNode<Primitive> Cond, [1]: CNode_900, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_867:CNode_902{[0]: ValueNode<Primitive> Switch, [1]: CNode_901, [2]: ValueNode<FuncGraph> check_axis_valid_903, [3]: ValueNode<FuncGraph> check_axis_valid_904}
#   8: @check_axis_valid_867:CNode_905{[0]: CNode_902}
#   9: @check_axis_valid_867:CNode_906{[0]: ValueNode<Primitive> Return, [1]: CNode_905}


subgraph attr:
subgraph instance: canonicalize_axis_865 : 000001EBEE5EE980
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @canonicalize_axis_865 parent: [subgraph @canonicalize_axis_833]() {
  Return(%para144_x_rank)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @canonicalize_axis_865:CNode_907{[0]: ValueNode<Primitive> Return, [1]: param_x_rank}


subgraph attr:
subgraph instance: canonicalize_axis_866 : 000001EBEE5EA9C0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @canonicalize_axis_866() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @canonicalize_axis_866:CNode_908{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: canonicalize_axis_873 : 000001EBEE5EB460
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @canonicalize_axis_873 parent: [subgraph @canonicalize_axis_833]() {
  Return(%para143_axis)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_873:CNode_909{[0]: ValueNode<Primitive> Return, [1]: param_axis}


subgraph attr:
subgraph instance: canonicalize_axis_874 : 000001EBEE5EAF10
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @canonicalize_axis_874 parent: [subgraph @canonicalize_axis_833]() {
  %1(CNode_862) = $(canonicalize_axis_833):S_Prim_not_equal(%para144_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_863) = $(canonicalize_axis_833):Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_864) = $(canonicalize_axis_833):Switch(%2, @canonicalize_axis_865, @canonicalize_axis_866)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = $(canonicalize_axis_833):%3()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_910) = S_Prim_add(%para143_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_874:CNode_910{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_axis, [2]: ndim}
#   2: @canonicalize_axis_874:CNode_911{[0]: ValueNode<Primitive> Return, [1]: CNode_910}


subgraph attr:
after_block : 1
subgraph instance: flatten_882 : 000001EBEE5E8F30
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_882 parent: [subgraph @flatten_810]() {
  %1(x_rank) = $(flatten_663):S_Prim_Rank(%para134_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(flatten_810):call @canonicalize_axis_833(%para99_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = $(flatten_810):call @canonicalize_axis_833(%para100_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_912) = S_Prim_equal(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  %5(CNode_913) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  %6(CNode_914) = Switch(%5, @flatten_915, @flatten_916)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  %7(CNode_917) = %6()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @flatten_882:CNode_912{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: idx, [2]: end_dim}
#   2: @flatten_882:CNode_913{[0]: ValueNode<Primitive> Cond, [1]: CNode_912, [2]: ValueNode<BoolImm> false}
#   3: @flatten_882:CNode_914{[0]: ValueNode<Primitive> Switch, [1]: CNode_913, [2]: ValueNode<FuncGraph> flatten_915, [3]: ValueNode<FuncGraph> flatten_916}
#   4: @flatten_882:CNode_917{[0]: CNode_914}
#   5: @flatten_882:CNode_918{[0]: ValueNode<Primitive> Return, [1]: CNode_917}


subgraph attr:
after_block : 1
subgraph instance: check_dim_valid_891 : 000001EBEE5F0960
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_891() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @check_dim_valid_891:CNode_919{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
subgraph instance: check_axis_valid_903 : 000001EBEE5ED440
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_903() {
  %1(CNode_920) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @check_axis_valid_903:CNode_920{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @check_axis_valid_903:CNode_921{[0]: ValueNode<Primitive> Return, [1]: CNode_920}


subgraph attr:
subgraph instance: check_axis_valid_904 : 000001EBEE5EC450
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_904() {
  %1(CNode_923) = call @check_axis_valid_922()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_904:CNode_923{[0]: ValueNode<FuncGraph> check_axis_valid_922}
#   2: @check_axis_valid_904:CNode_924{[0]: ValueNode<Primitive> Return, [1]: CNode_923}


subgraph attr:
subgraph instance: check_axis_valid_898 : 000001EBEE5EC9A0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_898 parent: [subgraph @check_axis_valid_867]() {
  %1(CNode_894) = $(check_axis_valid_867):S_Prim_negative(%para148_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_895) = $(check_axis_valid_867):S_Prim_less(%para147_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_898:CNode_925{[0]: ValueNode<Primitive> Return, [1]: CNode_895}


subgraph attr:
subgraph instance: check_axis_valid_899 : 000001EBEE5EEED0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_899 parent: [subgraph @check_axis_valid_867]() {
  %1(CNode_926) = S_Prim_greater_equal(%para147_axis, %para148_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_899:CNode_926{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @check_axis_valid_899:CNode_927{[0]: ValueNode<Primitive> Return, [1]: CNode_926}


subgraph attr:
subgraph instance: flatten_915 : 000001EBEE5EDEE0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_915 parent: [subgraph @flatten_663]() {
  Return(%para134_phi_input)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1763/        return input/
}
# Order:
#   1: @flatten_915:CNode_928{[0]: ValueNode<Primitive> Return, [1]: param_phi_input}


subgraph attr:
subgraph instance: flatten_916 : 000001EBEE5E9480
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_916 parent: [subgraph @flatten_810]() {
  %1(CNode_930) = call @flatten_929()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @flatten_916:CNode_930{[0]: ValueNode<FuncGraph> flatten_929}
#   2: @flatten_916:CNode_931{[0]: ValueNode<Primitive> Return, [1]: CNode_930}


subgraph attr:
after_block : 1
subgraph instance: check_axis_valid_922 : 000001EBEE5ECEF0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_922() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @check_axis_valid_922:CNode_932{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: flatten_929 : 000001EBEE5E89E0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_929 parent: [subgraph @flatten_810]() {
  %1(x_rank) = $(flatten_663):S_Prim_Rank(%para134_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(flatten_810):call @canonicalize_axis_833(%para99_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(CNode_934) = call @flatten_933(%2, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @flatten_929:CNode_935{[0]: ValueNode<Primitive> Return, [1]: CNode_934}
#   2: @flatten_929:CNode_934{[0]: ValueNode<FuncGraph> flatten_933, [1]: idx, [2]: ValueNode<Int64Imm> 1}


subgraph attr:
is_while_header : 1
subgraph instance: flatten_933 : 000001EBEE5E99D0
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_933 parent: [subgraph @flatten_810](%para149_, %para150_) {
  %1(x_rank) = $(flatten_663):S_Prim_Rank(%para134_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(end_dim) = $(flatten_810):call @canonicalize_axis_833(%para100_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %3(CNode_936) = S_Prim_less_equal(%para149_phi_idx, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
  %4(force_while_cond_CNode_936) = Cond(%3, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
  %5(CNode_937) = Switch(%4, @flatten_938, @flatten_939)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
  %6(CNode_940) = %5()
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @flatten_933:CNode_936{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less_equal, [1]: param_phi_idx, [2]: end_dim}
#   2: @flatten_933:force_while_cond_CNode_936{[0]: ValueNode<Primitive> Cond, [1]: CNode_936, [2]: ValueNode<BoolImm> true}
#   3: @flatten_933:CNode_937{[0]: ValueNode<Primitive> Switch, [1]: force_while_cond_CNode_936, [2]: ValueNode<FuncGraph> flatten_938, [3]: ValueNode<FuncGraph> flatten_939}
#   4: @flatten_933:CNode_940{[0]: CNode_937}
#   5: @flatten_933:CNode_941{[0]: ValueNode<Primitive> Return, [1]: CNode_940}


subgraph attr:
subgraph instance: flatten_938 : 000001EBEE5ED990
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_938 parent: [subgraph @flatten_933]() {
  %1(idx) = S_Prim_add(%para149_phi_idx, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1769/        idx += 1/
  %2(x_shape) = $(flatten_663):S_Prim_Shape(%para134_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1747/    x_shape = shape_(input)/
  %3(CNode_942) = S_Prim_getitem(%2, %para149_phi_idx)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1768/        dim_length *= x_shape[idx]/
  %4(dim_length) = S_Prim_mul(%para150_phi_dim_length, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1768/        dim_length *= x_shape[idx]/
  %5(CNode_943) = call @flatten_933(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @flatten_938:CNode_942{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: param_phi_idx}
#   2: @flatten_938:dim_length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_phi_dim_length, [2]: CNode_942}
#   3: @flatten_938:idx{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_phi_idx, [2]: ValueNode<Int64Imm> 1}
#   4: @flatten_938:CNode_944{[0]: ValueNode<Primitive> Return, [1]: CNode_943}
#   5: @flatten_938:CNode_943{[0]: ValueNode<FuncGraph> flatten_933, [1]: idx, [2]: dim_length}


subgraph attr:
subgraph instance: flatten_939 : 000001EBEE5E9F20
# In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_939 parent: [subgraph @flatten_933]() {
  %1(x_shape) = $(flatten_663):S_Prim_Shape(%para134_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1747/    x_shape = shape_(input)/
  %2(x_rank) = $(flatten_663):S_Prim_Rank(%para134_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %3(idx) = $(flatten_810):call @canonicalize_axis_833(%para99_start_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %4(CNode_945) = S_Prim_make_slice(None, %3, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %5(CNode_946) = S_Prim_getitem(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %6(CNode_947) = S_Prim_MakeTuple(%para150_phi_dim_length)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %7(CNode_948) = S_Prim_add(%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %8(end_dim) = $(flatten_810):call @canonicalize_axis_833(%para100_end_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %9(CNode_949) = S_Prim_add(%8, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %10(CNode_950) = S_Prim_make_slice(%9, None, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %11(CNode_951) = S_Prim_getitem(%1, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %12(new_shape) = S_Prim_add(%7, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %13(CNode_952) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para134_phi_input, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1771/    return reshape_(input, new_shape)/
  Return(%13)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-SimpleCNN/flatten-Flatten)
      # In file D:\Anaconda3\envs\env1\lib\site-packages\mindspore\ops\function\array_func.py:1771/    return reshape_(input, new_shape)/
}
# Order:
#   1: @flatten_939:CNode_945{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: idx, [3]: ValueNode<None> None}
#   2: @flatten_939:CNode_946{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_945}
#   3: @flatten_939:CNode_947{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_phi_dim_length}
#   4: @flatten_939:CNode_948{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_946, [2]: CNode_947}
#   5: @flatten_939:CNode_949{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: end_dim, [2]: ValueNode<Int64Imm> 1}
#   6: @flatten_939:CNode_950{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: CNode_949, [2]: ValueNode<None> None, [3]: ValueNode<None> None}
#   7: @flatten_939:CNode_951{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_950}
#   8: @flatten_939:new_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_948, [2]: CNode_951}
#   9: @flatten_939:CNode_952{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_phi_input, [2]: new_shape}
#  10: @flatten_939:CNode_953{[0]: ValueNode<Primitive> Return, [1]: CNode_952}


